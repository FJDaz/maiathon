# =============================================================================
# ‚ö° √âVALUATION INCR√âMENTALE - Fil de l'Eau (Optimisation Inf√©rence)
# =============================================================================
# üéÆ Cellule Colab : √âvaluation Incr√©mentale Ma√Øeuthon
# √Ä ajouter APR√àS la cellule /evaluate (Ma√Øeuthon) et AVANT la cellule Lancement Serveur
# =============================================================================

# Import n√©cessaire pour query parameter et mod√®les Pydantic
from fastapi import Query
from pydantic import BaseModel

# D√©finir EvaluateRequest (d√©j√† d√©fini dans cellule Ma√Øeuthon, mais on le red√©finit ici pour autonomie)
# Si la cellule Ma√Øeuthon a √©t√© ex√©cut√©e, cette d√©finition sera √©cras√©e (pas de conflit)
class EvaluateRequest(BaseModel):
    dialogue: str
    score_front: int

# Prompt √©valuation incr√©mentale (court, rapide)
PROMPT_EVALUATION_INCREMENTAL = """√âvalue rapidement (0-10) :
- Compr√©hension : Comprend-il mes id√©es ?
- Coop√©ration : Coop√®re-t-il dans le dialogue ?
- Progression : Sa pens√©e progresse-t-elle ?

Dialogue r√©cent (2 derniers √©changes) :
{dialogue_recent}

IMPORTANT: R√©ponds UNIQUEMENT avec un JSON valide, AUCUNE prose avant ou apr√®s.

Format JSON strict :
{{
 "comprehension": X,
 "cooperation": Y,
 "progression": Z,
 "total": X+Y+Z
}}"""

# Stockage scores incr√©mentaux (en m√©moire)
# Structure : {dialogue_id: [scores_√©change_2, scores_√©change_4, ...]}
incremental_scores = {}

def evaluer_incremental(dialogue: str, debug: bool = False, return_raw: bool = False):
    """
    √âvaluation l√©g√®re au fil de l'eau (tous les 2 √©changes)
    - Prompt court (2 derniers √©changes seulement)
    - Temp√©rature basse (0.1) - Strict pour JSON
    - Max tokens r√©duit (100) - Garantit un JSON complet
    - Pas de message final - Gain de temps
    
    Args:
        dialogue: Le dialogue √† √©valuer
        debug: Si True, affiche la r√©ponse brute du mod√®le pour diagnostic (dans les logs Colab)
        return_raw: Si True, retourne aussi la r√©ponse brute du mod√®le dans le tuple
    
    Returns:
        dict: Les scores √©valu√©s si return_raw=False
        tuple: (dict scores, str raw_response) si return_raw=True
    """
    # Extraire les 2 derniers √©changes seulement (4 lignes : √âl√®ve + Spinoza x2)
    lines = [l.strip() for l in dialogue.split('\n') if l.strip()]
    if len(lines) > 4:
        recent_exchanges = '\n'.join(lines[-4:])  # 2 derniers √©changes
    else:
        recent_exchanges = dialogue
    
    prompt_eval = PROMPT_EVALUATION_INCREMENTAL.format(dialogue_recent=recent_exchanges)
    
    # Formatage Mistral
    prompt_eval_formatted = f"<s>[INST] {prompt_eval} [/INST]"
    
    inputs = tokenizer(prompt_eval_formatted, return_tensors="pt").to(model.device)
    input_length = inputs['input_ids'].shape[1]
    
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    # Inf√©rence rapide (temp√©rature basse, tokens r√©duits)
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,  # Augment√© pour garantir un JSON complet (√©tait 50)
            temperature=0.1,    # Strict pour JSON
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens = outputs[0][input_length:]
    reponse_eval = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    # Debug: afficher la r√©ponse brute si demand√©
    if debug:
        print(f"üîç [DEBUG] R√©ponse brute du mod√®le: {reponse_eval[:500]}")
    
    # Parser JSON (am√©lior√© pour capturer m√™me avec texte autour)
    # Essayer plusieurs strat√©gies de parsing
    details_model = None
    
    # Strat√©gie 1: Pattern sp√©cifique avec tous les champs requis
    # Accepter les variantes avec/sans accents
    json_pattern = r'\{[^{}]*"(?:comprehension|compr√©hension|compr√©sentation)"[^{}]*"(?:cooperation|coop√©ration)"[^{}]*"progression"[^{}]*"total"[^{}]*\}'
    json_match = re.search(json_pattern, reponse_eval, re.DOTALL)
    
    if json_match:
        try:
            json_str = json_match.group(0)
            # Nettoyer le JSON (enlever les retours √† la ligne en trop, etc.)
            json_str = json_str.strip()
            details_model = json.loads(json_str)
        except json.JSONDecodeError as e:
            # Debug: afficher la r√©ponse brute pour diagnostic
            if debug:
                print(f"‚ö†Ô∏è Erreur parsing JSON incr√©mental (strat√©gie 1): {e}")
                print(f"   JSON brut extrait: {json_match.group(0)[:300]}")
    
    # Strat√©gie 2: Si √©chec, chercher le premier bloc JSON valide
    if not details_model:
        json_pattern_fallback = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        json_match = re.search(json_pattern_fallback, reponse_eval, re.DOTALL)
        if json_match:
            try:
                json_str = json_match.group(0).strip()
                details_model = json.loads(json_str)
            except json.JSONDecodeError as e:
                if debug:
                    print(f"‚ö†Ô∏è Erreur parsing JSON incr√©mental (strat√©gie 2): {e}")
    
    # Strat√©gie 3: Si toujours √©chec, essayer de parser directement toute la r√©ponse
    if not details_model:
        try:
            details_model = json.loads(reponse_eval.strip())
        except json.JSONDecodeError:
            if debug:
                print(f"‚ö†Ô∏è Erreur parsing JSON incr√©mental (strat√©gie 3): Impossible de parser la r√©ponse")
                print(f"   R√©ponse compl√®te: {reponse_eval[:300]}")
    
    # Valider et compl√©ter le mod√®le
    if details_model and isinstance(details_model, dict):
        # Normaliser les cl√©s (g√©rer les accents : "compr√©hension" -> "comprehension")
        # Mapping des variantes possibles vers les cl√©s standard
        import unicodedata
        
        def normalize_key(key: str) -> str:
            """Normalise une cl√© en enlevant les accents et en minuscules"""
            # Enlever les accents
            key_normalized = unicodedata.normalize('NFD', key.lower())
            key_normalized = ''.join(c for c in key_normalized if unicodedata.category(c) != 'Mn')
            
            # Mapping sp√©cifique pour les fautes de frappe courantes
            mapping = {
                "comprehension": "comprehension",
                "compr√©hension": "comprehension",
                "compr√©sentation": "comprehension",  # Faute de frappe
                "cooperation": "cooperation",
                "coop√©ration": "cooperation",
                "progression": "progression",
                "total": "total"
            }
            
            return mapping.get(key_normalized, key_normalized)
        
        # Cr√©er un nouveau dict avec les cl√©s normalis√©es
        normalized_model = {}
        for key, value in details_model.items():
            normalized_key = normalize_key(key)
            # Si la cl√© normalis√©e existe d√©j√†, garder la valeur la plus √©lev√©e (ou la premi√®re)
            if normalized_key not in normalized_model:
                normalized_model[normalized_key] = value
            else:
                # Si deux cl√©s se normalisent vers la m√™me, prendre la valeur num√©rique la plus √©lev√©e
                if isinstance(value, (int, float)) and isinstance(normalized_model[normalized_key], (int, float)):
                    normalized_model[normalized_key] = max(value, normalized_model[normalized_key])
        
        details_model = normalized_model
        
        # Valider que tous les champs sont pr√©sents
        required_fields = ["comprehension", "cooperation", "progression", "total"]
        for field in required_fields:
            if field not in details_model:
                details_model[field] = 5 if field != "total" else 15
        
        # Valider les valeurs (doivent √™tre des nombres entre 0 et 10, ou total entre 0 et 30)
        for field in ["comprehension", "cooperation", "progression"]:
            if not isinstance(details_model[field], (int, float)) or not (0 <= details_model[field] <= 10):
                details_model[field] = 5
        
        # Recalculer total si n√©cessaire
        if "total" not in details_model or not isinstance(details_model["total"], (int, float)):
            details_model["total"] = details_model.get("comprehension", 5) + \
                                    details_model.get("cooperation", 5) + \
                                    details_model.get("progression", 5)
    else:
        # Valeurs par d√©faut si parsing √©choue
        # Toujours afficher dans les logs Colab pour diagnostic
        print(f"‚ö†Ô∏è JSON non trouv√© ou invalide dans r√©ponse incr√©mentale")
        print(f"   R√©ponse brute (premiers 500 chars): {reponse_eval[:500]}")
        details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
    
    # Retourner selon le mode demand√©
    if return_raw:
        return details_model, reponse_eval
    return details_model

# Endpoint FastAPI pour √©valuation incr√©mentale
@app.post("/evaluate/incremental")
def evaluate_incremental(
    req: EvaluateRequest,
    debug: bool = Query(False, description="Activer le mode debug pour voir la r√©ponse brute du mod√®le")
):
    """
    √âvaluation l√©g√®re au fil de l'eau (tous les 2 √©changes)
    - Appel√© par le frontend apr√®s chaque 2 √©changes
    - Stocke les scores pour l'√©valuation finale
    - Retourne seulement les scores (pas de message final)
    
    Param√®tre debug (query string):
        ?debug=true - Retourne aussi la r√©ponse brute du mod√®le dans la r√©ponse HTTP
                      Exemple: POST /evaluate/incremental?debug=true
    """
    # √âvaluer le dialogue r√©cent (avec debug si demand√© via query param ?debug=true)
    if debug:
        details_model, raw_response = evaluer_incremental(req.dialogue, debug=True, return_raw=True)
    else:
        details_model = evaluer_incremental(req.dialogue, debug=False, return_raw=False)
        raw_response = None
    
    # Stocker pour l'√©valuation finale
    # Utiliser un ID simple bas√© sur le hash du dialogue
    # En production, utiliser un vrai ID de session
    dialogue_id = hash(req.dialogue)
    
    if dialogue_id not in incremental_scores:
        incremental_scores[dialogue_id] = []
    
    incremental_scores[dialogue_id].append({
        "scores": details_model,
        "exchange_count": len(incremental_scores[dialogue_id]) + 1
    })
    
    # Pr√©parer la r√©ponse
    response = {
        "scores": details_model,
        "exchange_count": len(incremental_scores[dialogue_id]),
        "accumulated": len(incremental_scores[dialogue_id]) > 0
    }
    
    # Ajouter la r√©ponse brute si debug demand√©
    if debug and raw_response:
        response["debug"] = {
            "raw_model_response": raw_response[:500],  # Premiers 500 chars
            "parsing_success": details_model.get("total", 0) != 15  # True si parsing r√©ussi (pas valeurs par d√©faut)
        }
    
    return response

print("‚úÖ Endpoint /evaluate/incremental cr√©√© pour √©valuation au fil de l'eau")

