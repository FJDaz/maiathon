{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéì Spinoza Secours - Notebook Colab\n",
        "\n",
        "**Mod√®le :** Mistral 7B + LoRA Fine-tuned  \n",
        "**Prompt :** Syst√®me hybride optimis√© (~250-300 tokens)  \n",
        "**API :** FastAPI + ngrok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Cellule 1 : Installation D√©pendances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pyngrok fastapi uvicorn transformers peft accelerate bitsandbytes torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Cellule 2 : Imports + Configuration\n",
        "\n",
        "**üîê Configuration des secrets (√† faire une seule fois) :**\n",
        "\n",
        "1. Clique sur l'ic√¥ne üîë **Secrets** dans le panneau de gauche de Colab\n",
        "2. Ajoute deux secrets :\n",
        "   - **`ngrok`** : Ton token ngrok (https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "   - **`HuggingFaceToken`** : Ton token Hugging Face (https://huggingface.co/settings/tokens)\n",
        "\n",
        "Les tokens seront r√©cup√©r√©s automatiquement via `userdata.get()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import threading\n",
        "import torch\n",
        "from typing import Dict, List, Optional\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration mod√®le\n",
        "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "ADAPTER_MODEL = \"FJDaz/mistral-7b-philosophes-lora\"\n",
        "\n",
        "# R√©cup√©ration des tokens depuis Colab Secrets\n",
        "# ‚ö†Ô∏è Configure tes secrets dans Colab : üîë Secrets ‚Üí Ajouter 'ngrok' et 'HuggingFaceToken'\n",
        "try:\n",
        "    NGROK_TOKEN = userdata.get('ngrok')\n",
        "    print(\"‚úÖ Token ngrok r√©cup√©r√© depuis Colab Secrets\")\n",
        "except:\n",
        "    print(\"‚ùå Token ngrok non trouv√© ! Configure le secret 'ngrok' dans Colab Secrets\")\n",
        "    NGROK_TOKEN = None\n",
        "\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HuggingFaceToken')\n",
        "    print(\"‚úÖ Token Hugging Face r√©cup√©r√© depuis Colab Secrets\")\n",
        "except:\n",
        "    print(\"‚ùå Token Hugging Face non trouv√© ! Configure le secret 'HuggingFaceToken' dans Colab Secrets\")\n",
        "    HF_TOKEN = None\n",
        "\n",
        "# Configuration ngrok\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è ngrok ne sera pas configur√© sans token\")\n",
        "\n",
        "# Gestion des ports : Port al√©atoire pour √©viter les conflits\n",
        "PORT = random.randint(8001, 9000)\n",
        "print(f\"üîå Port s√©lectionn√© : {PORT}\")\n",
        "\n",
        "# Tuer processus sur le port s√©lectionn√© si existant\n",
        "import subprocess\n",
        "try:\n",
        "    result = subprocess.run(f\"lsof -ti:{PORT}\", shell=True, capture_output=True, text=True)\n",
        "    if result.stdout.strip():\n",
        "        subprocess.run(f\"kill -9 {result.stdout.strip()}\", shell=True)\n",
        "        print(f\"üßπ Port {PORT} lib√©r√©\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"‚úÖ Imports et configuration OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Cellule 3 : Prompt Syst√®me Hybride\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt syst√®me hybride optimis√© (~250 tokens)\n",
        "SYSTEM_PROMPT_SPINOZA = \"\"\"Tu ES Spinoza incarn√©. Tu dialogues avec un √©l√®ve de Terminale en premi√®re personne.\n",
        "\n",
        "STYLE SPINOZIEN :\n",
        "- G√©om√©trie des affects : r√©v√®le causes n√©cessaires, d√©duis\n",
        "- Dieu = Nature\n",
        "- Vocabulaire : conatus, affects, puissance d'agir, servitude\n",
        "\n",
        "SCH√àMES LOGIQUES :\n",
        "- Identit√© : Libert√© = Connaissance n√©cessit√©\n",
        "- Causalit√© : Tout a cause n√©cessaire\n",
        "- Implication : Joie ‚Üí augmentation puissance\n",
        "\n",
        "M√âTHODE :\n",
        "1. R√©v√®le n√©cessit√© causale\n",
        "2. Distingue servitude (ignorance) vs libert√© (connaissance)\n",
        "3. Exemples concrets modernes\n",
        "\n",
        "TRANSITIONS (VARIE) :\n",
        "- \"Donc\", \"mais alors\", \"Imagine\", \"Cela implique\"\n",
        "- \"Pourtant\", \"Sauf que\", \"C'est contradictoire\"\n",
        "\n",
        "R√àGLES :\n",
        "- Tutoie (tu/ton/ta)\n",
        "- Concis (2-3 phrases MAX)\n",
        "- Questionne au lieu d'affirmer\n",
        "- Ne parle JAMAIS de toi √† la 3√®me personne. Tu ES Spinoza.\n",
        "- Vocabulaire MODERNE : utilise le langage d'aujourd'hui, √©vite les archa√Øsmes\n",
        "- Si tu utilises \"conatus\", \"affects\", explique-les avec des mots d'aujourd'hui\"\"\"\n",
        "\n",
        "INSTRUCTIONS_CONTEXTUELLES = {\n",
        "    \"confusion\": \"L'√©l√®ve est confus ‚Üí Donne UNE analogie concr√®te simple en utilisant tes sch√®mes logiques.\",\n",
        "    \"resistance\": \"L'√©l√®ve r√©siste ‚Üí R√©v√®le contradiction avec 'mais alors' et tes sch√®mes logiques.\",\n",
        "    \"accord\": \"L'√©l√®ve est d'accord ‚Üí Valide puis AVANCE logiquement avec 'Donc' et tes sch√®mes logiques.\",\n",
        "    \"neutre\": \"√âl√®ve neutre ‚Üí Pose question pour faire r√©fl√©chir en utilisant tes sch√®mes logiques.\"\n",
        "}\n",
        "\n",
        "INSTRUCTION_RAG = \"\"\"\n",
        "UTILISATION CONNAISSANCES :\n",
        "- Tu connais l'√âthique de Spinoza\n",
        "- Cite implicitement (\"comme je l'ai montr√©...\", \"dans mon ≈ìuvre...\")\n",
        "- Reformule dans TON style (premi√®re personne, lyc√©en)\n",
        "- Ne r√©cite pas : extrais id√©es et reformule naturellement\n",
        "\"\"\"\n",
        "\n",
        "def construire_prompt_complet(contexte: str, use_rag_instruction: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Construit le prompt complet optimis√©\n",
        "    \n",
        "    Args:\n",
        "        contexte: \"accord\", \"confusion\", \"resistance\", \"neutre\"\n",
        "        use_rag_instruction: Si True, ajoute instructions RAG\n",
        "    \n",
        "    Returns:\n",
        "        Prompt syst√®me complet (~250-300 tokens)\n",
        "    \"\"\"\n",
        "    prompt = SYSTEM_PROMPT_SPINOZA\n",
        "    \n",
        "    # Ajouter instruction contextuelle\n",
        "    if contexte in INSTRUCTIONS_CONTEXTUELLES:\n",
        "        prompt += f\"\\n\\n{INSTRUCTIONS_CONTEXTUELLES[contexte]}\"\n",
        "    \n",
        "    # Ajouter instruction RAG (optionnel)\n",
        "    if use_rag_instruction:\n",
        "        prompt += f\"\\n\\n{INSTRUCTION_RAG}\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "print(\"‚úÖ Prompt syst√®me hybride charg√©\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Cellule 4 : D√©tection Contexte + Post-Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detecter_contexte(user_input: str) -> str:\n",
        "    \"\"\"D√©tecte le contexte de la r√©ponse utilisateur\"\"\"\n",
        "    text_lower = user_input.lower()\n",
        "    \n",
        "    # Accord\n",
        "    if any(word in text_lower for word in ['oui', \"d'accord\", 'exact', 'ok', 'voil√†', 'tout √† fait']):\n",
        "        return \"accord\"\n",
        "    \n",
        "    # Confusion\n",
        "    if any(phrase in text_lower for phrase in ['comprends pas', 'vois pas', \"c'est quoi\", 'je sais pas', 'pourquoi', 'rapport']):\n",
        "        return \"confusion\"\n",
        "    \n",
        "    # R√©sistance\n",
        "    if any(word in text_lower for word in ['mais', 'non', \"pas d'accord\", 'faux', \"n'importe quoi\", 'je peux']):\n",
        "        return \"resistance\"\n",
        "    \n",
        "    return \"neutre\"\n",
        "\n",
        "def nettoyer_reponse(text: str) -> str:\n",
        "    \"\"\"Nettoie la r√©ponse g√©n√©r√©e\"\"\"\n",
        "    text = re.sub(r'\\([^)]*[Aa]ttends[^)]*\\)', '', text)\n",
        "    text = re.sub(r'\\([^)]*[Pp]oursuis[^)]*\\)', '', text)\n",
        "    text = re.sub(r'\\([^)]*[Dd]onne[^)]*\\)', '', text)\n",
        "    text = re.sub(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\s+([.!?])', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def limiter_phrases(text: str, max_phrases: int = 4) -> str:\n",
        "    \"\"\"Limite le nombre de phrases\"\"\"\n",
        "    phrases = re.split(r'[.!?]+\\s+', text)\n",
        "    phrases = [p.strip() for p in phrases if p.strip()]\n",
        "    if len(phrases) <= max_phrases:\n",
        "        return text\n",
        "    return '. '.join(phrases[:max_phrases]) + '.'\n",
        "\n",
        "print(\"‚úÖ D√©tection contexte + Post-processing OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Cellule 5 : Chargement Mod√®le\n",
        "\n",
        "**‚ö†Ô∏è Code reproduit exactement depuis app.py - Ne pas modifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def load_model():\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    print(f\"üñ•Ô∏è GPU disponible: {has_gpu}\")\n",
        "\n",
        "    if has_gpu:\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        device_map = \"auto\"\n",
        "        torch_dtype = torch.bfloat16\n",
        "    else:\n",
        "        quantization_config = None\n",
        "        device_map = \"cpu\"\n",
        "        torch_dtype = torch.float32\n",
        "\n",
        "    print(f\"üîÑ Chargement Mistral 7B ({'4-bit GPU' if has_gpu else 'FP32 CPU'})...\")\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=device_map,\n",
        "        torch_dtype=torch_dtype,\n",
        "        token=HF_TOKEN,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    print(\"üîÑ Chargement tokenizer...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        token=HF_TOKEN,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"üîÑ Application LoRA Spinoza_Secours...\")\n",
        "\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        ADAPTER_MODEL,\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Mod√®le Mistral 7B + LoRA charg√©!\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Charger le mod√®le\n",
        "model, tokenizer = load_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí¨ Cellule 6 : Fonction spinoza_repond()\n",
        "\n",
        "**Utilise le prompt syst√®me hybride optimis√©**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Historique conversation (format: [user, assistant])\n",
        "conversation_history = []\n",
        "\n",
        "def spinoza_repond(message: str) -> str:\n",
        "    \"\"\"\n",
        "    G√©n√®re une r√©ponse de Spinoza avec prompt hybride adaptatif\n",
        "    \n",
        "    Args:\n",
        "        message: Message de l'utilisateur\n",
        "    \n",
        "    Returns:\n",
        "        R√©ponse de Spinoza nettoy√©e\n",
        "    \"\"\"\n",
        "    global conversation_history\n",
        "    \n",
        "    # D√©tecter contexte\n",
        "    contexte = detecter_contexte(message)\n",
        "    \n",
        "    # Construire prompt adaptatif (RAG d√©sactiv√© par d√©faut - trop embrouillant)\n",
        "    system_prompt = construire_prompt_complet(contexte, use_rag_instruction=False)\n",
        "    \n",
        "    # Formatage Mistral style\n",
        "    prompt_parts = [f\"<s>[INST] {system_prompt}\\n\\n\"]\n",
        "    \n",
        "    # Ajouter historique (4 derniers √©changes max)\n",
        "    for entry in conversation_history[-4:]:\n",
        "        prompt_parts.append(f\"{entry[0]} [/INST] {entry[1]}</s>[INST] \")\n",
        "    \n",
        "    prompt_parts.append(f\"{message} [/INST]\")\n",
        "    text = \"\".join(prompt_parts)\n",
        "    \n",
        "    # Tokenization\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "    \n",
        "    # G√©n√©ration\n",
        "    device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype = torch.bfloat16 if device_type == \"cuda\" else torch.float32\n",
        "    \n",
        "    with torch.autocast(device_type=device_type, dtype=dtype):\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # D√©codage\n",
        "    new_tokens = outputs[0][input_length:]\n",
        "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    # Post-processing\n",
        "    response = nettoyer_reponse(response)\n",
        "    response = limiter_phrases(response, max_phrases=3)\n",
        "    \n",
        "    # Mettre √† jour historique\n",
        "    conversation_history.append([message, response])\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Fonction spinoza_repond() cr√©√©e\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåê Cellule 7 : API FastAPI + ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Questions d'amorce\n",
        "QUESTIONS_BAC = [\n",
        "    \"La libert√© est-elle une illusion ?\",\n",
        "    \"Suis-je esclave de mes d√©sirs ?\",\n",
        "    \"Puis-je ma√Ætriser mes √©motions ?\",\n",
        "    \"La joie procure-t-elle un pouvoir ?\",\n",
        "    \"Peut-on d√©sirer sans souffrir ?\",\n",
        "]\n",
        "\n",
        "# Mod√®les Pydantic\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "    history: Optional[List[List[str]]] = None\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    reply: str\n",
        "    history: List[List[str]]\n",
        "\n",
        "# API FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # ‚ö†Ô∏è √Ä restreindre en production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    \"\"\"Endpoint racine - Informations sur l'API\"\"\"\n",
        "    return {\n",
        "        \"name\": \"Spinoza Secours API\",\n",
        "        \"model\": \"Mistral 7B + LoRA\",\n",
        "        \"status\": \"running\",\n",
        "        \"endpoints\": {\n",
        "            \"health\": \"GET /health\",\n",
        "            \"init\": \"GET /init\",\n",
        "            \"chat\": \"POST /chat\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\", \"model\": \"Mistral 7B + LoRA\"}\n",
        "\n",
        "@app.get(\"/init\")\n",
        "def init():\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    question = random.choice(QUESTIONS_BAC)\n",
        "    greeting = f\"Bonjour ! Je suis Spinoza. Discutons :\\n\\n**{question}**\\n\\nQu'en penses-tu ?\"\n",
        "    return {\n",
        "        \"greeting\": greeting,\n",
        "        \"history\": [[None, greeting]]\n",
        "    }\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "def chat(req: ChatRequest):\n",
        "    global conversation_history\n",
        "    \n",
        "    # Mettre √† jour historique si fourni\n",
        "    if req.history:\n",
        "        conversation_history = req.history\n",
        "    \n",
        "    # G√©n√©rer r√©ponse\n",
        "    reply = spinoza_repond(req.message)\n",
        "    \n",
        "    return {\n",
        "        \"reply\": reply,\n",
        "        \"history\": conversation_history\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ API FastAPI cr√©√©e\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Cellule 8 : Lancement Serveur + ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lancer FastAPI en background avec port dynamique\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"error\")\n",
        "\n",
        "thread = threading.Thread(target=run_server, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Attendre que le serveur d√©marre\n",
        "import time\n",
        "time.sleep(3)\n",
        "\n",
        "# Tunnel ngrok sur le port dynamique\n",
        "tunnel = ngrok.connect(PORT)\n",
        "# Extraire l'URL publique depuis l'objet NgrokTunnel\n",
        "# L'objet NgrokTunnel a une m√©thode public_url ou on peut extraire depuis str()\n",
        "if hasattr(tunnel, 'public_url'):\n",
        "    public_url = tunnel.public_url\n",
        "elif hasattr(tunnel, 'data') and 'public_url' in tunnel.data:\n",
        "    public_url = tunnel.data['public_url']\n",
        "else:\n",
        "    # Fallback : extraire depuis la repr√©sentation string\n",
        "    import re\n",
        "    url_match = re.search(r'https://[^\"]+\\.ngrok[^\"]+', str(tunnel))\n",
        "    public_url = url_match.group(0) if url_match else str(tunnel)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"üöÄ API PUBLIQUE : {public_url}\")\n",
        "print(f\"üì° Health : {public_url}/health\")\n",
        "print(f\"üí¨ Init : {public_url}/init\")\n",
        "print(f\"üí¨ Chat : POST {public_url}/chat\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n‚úÖ Serveur lanc√© sur le port {PORT} !\")\n",
        "print(f\"üìã Copie cette URL dans index_spinoza.html : {public_url}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
