# üìà Strat√©gie de Scaling vers Qwen 14B + LoRA

**Date :** 28 novembre 2025  
**Migration :** Mistral 7B ‚Üí Qwen 14B + LoRA (SNB)  
**Objectif :** Planifier la mont√©e en charge future

---

## üéØ Vue d'Ensemble

### Mod√®les Compar√©s

| Crit√®re | Mistral 7B | Qwen 14B | Diff√©rence |
|---------|------------|----------|------------|
| **Taille mod√®le** | ~14 GB | ~28 GB | **+100%** |
| **VRAM n√©cessaire (4-bit)** | ~6-8 GB | ~12-14 GB | **+75-100%** |
| **VRAM n√©cessaire (8-bit)** | ~10-12 GB | ~20-24 GB | **+100%** |
| **VRAM recommand√©e** | 16-24 GB | **32-48 GB** | **+100%** |
| **Latence inference** | ~1-2s | ~2-4s | **+100%** |
| **Co√ªt/h (RTX 4090)** | $0.27-0.29 | $0.27-0.29 | **Identique** |

---

## ‚ùå Limitation Vast.ai : Changement de GPU

### R√©ponse Courte : **NON, impossible de changer le GPU dans une instance existante**

**Raisons techniques :**
1. **Instance = GPU sp√©cifique** : Chaque instance Vast.ai est li√©e √† un GPU sp√©cifique au moment de la cr√©ation
2. **Pas de "GPU swap"** : Vast.ai ne permet pas de changer le GPU d'une instance en cours d'ex√©cution
3. **Container Disk li√© au GPU** : Le stockage est attach√© √† l'instance GPU sp√©cifique

**Ce qui est possible :**
- ‚úÖ Modifier les variables d'environnement
- ‚úÖ Modifier le code Docker (via GitHub)
- ‚úÖ Red√©marrer l'instance
- ‚úÖ Changer la taille du Container Disk (dans certaines limites)
- ‚ùå **Changer le GPU** (impossible)

---

## ‚úÖ Solutions pour Scaling vers Qwen 14B

### Option 1 : Nouvelle Instance (RECOMMAND√â) ‚≠ê‚≠ê‚≠ê

**Strat√©gie :** Cr√©er une nouvelle instance avec GPU adapt√©

**Avantages :**
- ‚úÖ Choix optimal du GPU (32-48GB VRAM)
- ‚úÖ Pas d'interruption de service (Mistral 7B continue)
- ‚úÖ Test en parall√®le avant migration
- ‚úÖ Rollback facile (garder les deux instances)
- ‚úÖ Configuration optimis√©e d√®s le d√©part

**Inconv√©nients :**
- ‚ö†Ô∏è Co√ªt double pendant transition (2 instances)
- ‚ö†Ô∏è N√©cessite mise √† jour frontend (changement d'URL)

**GPU Recommand√©s pour Qwen 14B :**

| GPU | VRAM | Co√ªt/h | Recommandation |
|-----|------|--------|----------------|
| **RTX 4090** | 24 GB | $0.27-0.29 | ‚ö†Ô∏è **Limite** (4-bit seulement) |
| **A100 40GB** | 40 GB | $1.00-1.50 | ‚úÖ **Optimal** (4-bit ou 8-bit) |
| **A100 80GB** | 80 GB | $2.00-3.00 | ‚úÖ **Parfait** (8-bit, marge) |
| **RTX 6000 Ada** | 48 GB | $0.80-1.20 | ‚úÖ **Bon compromis** |

**Recommandation :**
- **Usage ponctuel :** RTX 4090 (24GB) avec quantization 4-bit ‚ö†Ô∏è Limite
- **Usage production :** A100 40GB ou RTX 6000 Ada 48GB ‚úÖ Optimal

---

### Option 2 : Template R√©utilisable ‚≠ê‚≠ê

**Strat√©gie :** Cr√©er un template g√©n√©rique r√©utilisable

**Avantages :**
- ‚úÖ Configuration Docker r√©utilisable
- ‚úÖ Variables d'environnement param√©trables
- ‚úÖ Migration rapide (cr√©er instance depuis template)

**Configuration Template :**

```yaml
# Template : spinoza-secours-generic
Variables d'environnement :
  - MODEL_NAME: mistralai/Mistral-7B-Instruct-v0.2  # ou Qwen/Qwen2.5-14B-Instruct
  - ADAPTER_NAME: FJDaz/mistral-7b-philosophes-lora  # ou FJDaz/qwen-14b-snb-lora
  - HF_TOKEN: [token]
  - PORT: 8000
  - QUANTIZATION: 4bit  # ou 8bit pour Qwen 14B
```

**Dockerfile g√©n√©rique :**
- M√™me structure
- Variables d'environnement pour mod√®le/adapter
- Code Python adaptatif

---

### Option 3 : Migration Progressive (A/B Testing) ‚≠ê‚≠ê‚≠ê

**Strat√©gie :** Tester Qwen 14B en parall√®le avant migration compl√®te

**√âtapes :**

1. **Phase 1 : D√©ploiement Qwen 14B (Test)**
   - Cr√©er nouvelle instance avec A100 40GB
   - D√©ployer Qwen 14B + LoRA
   - URL test : `spinoza-secours-qwen-test.vast.ai:8000`

2. **Phase 2 : Tests Comparatifs**
   - Comparer latence Mistral 7B vs Qwen 14B
   - Comparer qualit√© des r√©ponses
   - Tester charge (nombre d'utilisateurs simultan√©s)

3. **Phase 3 : Migration Progressive**
   - Option A : Frontend avec toggle Mistral/Qwen
   - Option B : Migration compl√®te apr√®s validation
   - Option C : Garder les deux (Mistral pour usage ponctuel, Qwen pour production)

4. **Phase 4 : Arr√™t Instance Mistral (si migration compl√®te)**
   - Arr√™ter instance Mistral 7B
   - Mettre √† jour frontend avec URL Qwen
   - Lib√©rer ressources

---

## üìã Plan de Migration Recommand√©

### √âtape 1 : Pr√©paration (Maintenant)

**Actions :**
- [x] Cr√©er template g√©n√©rique (Dockerfile + app_runpod.py)
- [ ] Ajouter variables d'environnement pour mod√®le/adapter
- [ ] Tester template avec Mistral 7B
- [ ] Documenter configuration

**Fichiers √† modifier :**
- `Backend/app_runpod.py` : Ajouter variables `MODEL_NAME`, `ADAPTER_NAME`
- `Backend/Dockerfile.runpod` : Reste identique
- `Backend/requirements.runpod.txt` : V√©rifier compatibilit√© Qwen

### √âtape 2 : D√©ploiement Qwen 14B (Quand pr√™t)

**Actions :**
1. **Cr√©er nouvelle instance Vast.ai**
   - GPU : A100 40GB ou RTX 6000 Ada 48GB
   - Container Disk : 100GB (Qwen 14B = ~28GB)
   - Template : Utiliser template g√©n√©rique
   - Variables : `MODEL_NAME=Qwen/Qwen2.5-14B-Instruct`, `ADAPTER_NAME=FJDaz/qwen-14b-snb-lora`

2. **D√©ployer et tester**
   - Build Docker : 5-10 min
   - T√©l√©chargement Qwen 14B : 15-20 min (~28GB)
   - Chargement GPU : 2-3 min
   - Tests endpoints : `/health`, `/init`, `/chat`, `/evaluate`

3. **Comparaison Mistral vs Qwen**
   - Latence : Mesurer temps de r√©ponse
   - Qualit√© : Tester dialogues r√©els
   - Stabilit√© : Test charge (10-20 utilisateurs simultan√©s)

### √âtape 3 : Migration (Apr√®s validation)

**Option A : Migration Compl√®te**
- Arr√™ter instance Mistral 7B
- Mettre √† jour frontend avec URL Qwen
- Monitorer performance

**Option B : Dual Deployment**
- Garder Mistral 7B pour usage ponctuel (co√ªt r√©duit)
- Utiliser Qwen 14B pour production (meilleure qualit√©)
- Frontend avec toggle ou routing intelligent

---

## üí∞ Co√ªts Compar√©s

### Mistral 7B (Actuel)
- **GPU :** RTX 4090 (24GB)
- **Co√ªt :** $0.27-0.29/h
- **Usage ponctuel (3h) :** ~$0.82
- **1 mois (24/7) :** ~$196

### Qwen 14B (Futur)
- **GPU :** A100 40GB (recommand√©)
- **Co√ªt :** $1.00-1.50/h
- **Usage ponctuel (3h) :** ~$3.00-4.50
- **1 mois (24/7) :** ~$720-1080

**Alternative RTX 4090 (limite) :**
- **GPU :** RTX 4090 (24GB) ‚ö†Ô∏è Limite
- **Co√ªt :** $0.27-0.29/h (identique)
- **Quantization :** 4-bit obligatoire
- **Risque :** OOM possible si contexte long

---

## üîß Modifications Code N√©cessaires

### 1. `app_runpod.py` - Variables d'environnement

```python
# Configuration
BASE_MODEL = os.getenv("MODEL_NAME", "mistralai/Mistral-7B-Instruct-v0.2")
ADAPTER_MODEL = os.getenv("ADAPTER_NAME", "FJDaz/mistral-7b-philosophes-lora")
QUANTIZATION = os.getenv("QUANTIZATION", "4bit")  # 4bit ou 8bit
```

### 2. `app_runpod.py` - Support Qwen

```python
# Formatage prompt selon mod√®le
if "qwen" in BASE_MODEL.lower():
    # Format Qwen
    prompt_format = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{message}<|im_end|>\n<|im_start|>assistant\n"
else:
    # Format Mistral
    prompt_format = f"<s>[INST] {system_prompt}\n\n{message} [/INST]"
```

### 3. `requirements.runpod.txt` - V√©rifier compatibilit√©

```txt
torch>=2.2.0
transformers>=4.40.0  # V√©rifier support Qwen
peft>=0.10.0
bitsandbytes>=0.43.0
accelerate>=0.28.0
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.5.0
slowapi>=0.1.9
```

---

## ‚úÖ Recommandations Finales

### Pour l'Instance Actuelle (RTX 4090 24GB)

**Mistral 7B :** ‚úÖ **Parfait**
- VRAM suffisante (6-8GB avec 4-bit)
- Marge confortable
- Performance optimale

**Qwen 14B :** ‚ö†Ô∏è **Limite**
- VRAM juste suffisante (12-14GB avec 4-bit)
- Pas de marge
- Risque OOM si contexte long
- **Recommandation :** Ne pas utiliser cette instance pour Qwen 14B

### Strat√©gie Recommand√©e

1. **Maintenant (Mistral 7B) :**
   - ‚úÖ Utiliser instance RTX 4090 24GB actuelle
   - ‚úÖ Container Disk 50GB
   - ‚úÖ Template g√©n√©rique (pr√©parer pour migration)

2. **Plus tard (Qwen 14B) :**
   - ‚úÖ Cr√©er **nouvelle instance** avec A100 40GB ou RTX 6000 Ada 48GB
   - ‚úÖ Container Disk 100GB
   - ‚úÖ R√©utiliser template g√©n√©rique
   - ‚úÖ Tester en parall√®le avant migration

3. **Migration :**
   - ‚úÖ A/B testing (Mistral vs Qwen)
   - ‚úÖ Migration progressive ou compl√®te selon r√©sultats
   - ‚úÖ Garder option de rollback (instance Mistral)

---

## üìù Checklist Migration Qwen 14B

### Pr√©paration
- [ ] Modifier `app_runpod.py` pour support variables d'environnement
- [ ] Tester template g√©n√©rique avec Mistral 7B
- [ ] V√©rifier compatibilit√© Qwen dans `requirements.runpod.txt`
- [ ] Documenter format prompts Qwen

### D√©ploiement
- [ ] Cr√©er nouvelle instance Vast.ai (A100 40GB ou RTX 6000 Ada 48GB)
- [ ] Configurer Container Disk 100GB
- [ ] D√©ployer avec variables Qwen
- [ ] Tester endpoints

### Validation
- [ ] Comparer latence Mistral vs Qwen
- [ ] Comparer qualit√© des r√©ponses
- [ ] Tester charge (utilisateurs simultan√©s)
- [ ] Valider stabilit√©

### Migration
- [ ] D√©cider : migration compl√®te ou dual deployment
- [ ] Mettre √† jour frontend (si migration compl√®te)
- [ ] Monitorer performance
- [ ] Documenter changements

---

## üîó R√©f√©rences

- **Plan migration actuel :** `docs/references/PLAN_MIGRATION_VAST_AI.md`
- **Template g√©n√©rique :** √Ä cr√©er
- **Documentation Qwen :** √Ä ajouter

---

**Conclusion :** ‚ùå **Impossible de changer le GPU dans une instance existante**. ‚úÖ **Cr√©er une nouvelle instance avec GPU adapt√© (A100 40GB ou RTX 6000 Ada 48GB) est la meilleure strat√©gie.**

