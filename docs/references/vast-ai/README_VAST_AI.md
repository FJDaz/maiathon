# Guide de D√©ploiement Vast.ai - Spinoza Secours

**Mod√®le :** Mistral 7B + LoRA  
**Budget :** ~$0.20-0.40/h (RTX 3090)  
**Usage :** Ponctuel (d√©mos/sessions)  
**D√©p√¥t minimum :** G√©n√©ralement 0‚Ç¨ (paiement √† l'usage)

---

## üìã Pr√©requis

1. **Compte Vast.ai** : Cr√©er un compte sur [vast.ai](https://vast.ai/)
2. **Token Hugging Face** : Obtenir un token avec acc√®s en lecture sur [Hugging Face](https://huggingface.co/settings/tokens)
3. **GitHub** (optionnel) : Si vous d√©ployez depuis un d√©p√¥t GitHub

---

## üöÄ √âtapes de D√©ploiement

### √âtape 1 : Pr√©parer les Fichiers

Les fichiers suivants sont d√©j√† pr√™ts dans le projet :
- `Dockerfile.runpod` - Dockerfile compatible Vast.ai
- `app_runpod.py` - Application FastAPI compl√®te
- `requirements.runpod.txt` - D√©pendances Python

### √âtape 2 : Cr√©er une Instance Vast.ai

1. **Se connecter √† Vast.ai**
   - Aller sur [vast.ai](https://vast.ai/)
   - Se connecter ou cr√©er un compte

2. **Cr√©er une nouvelle instance**
   - Cliquer sur **"Create"** ou **"New Instance"**
   - S√©lectionner **"Docker"** comme type d'instance

3. **Configurer l'instance**
   - **GPU** : **RTX 3090 (24GB) recommand√©** ‚≠ê
     - Co√ªt : $0.20-0.40/h (similaire ou moins cher que T4)
     - Performance : 2-3x plus rapide que T4
     - VRAM : 24GB (suffisant pour Mistral 7B en 4-bit)
     - Alternative : RTX 4090 si besoin de plus de performance ($0.35-0.60/h)
   - **Image Docker** : S√©lectionner **"Custom Dockerfile"** ou **"From GitHub"**
   - **Container Disk** : 50GB minimum (pour le mod√®le Mistral 7B)
   - **Port** : 8000 (expos√© automatiquement)

4. **Configurer les variables d'environnement**
   Dans l'interface Vast.ai, ajouter :
   ```
   HF_TOKEN=votre_token_huggingface
   PORT=8000
   ```

5. **Configurer le Dockerfile**
   - Si d√©ploiement depuis GitHub : Sp√©cifier le chemin `Backend/Dockerfile.runpod`
   - Si d√©ploiement direct : Copier le contenu de `Dockerfile.runpod`

### √âtape 3 : D√©ployer

1. **Lancer l'instance**
   - Cliquer sur **"Deploy"** ou **"Start"**
   - Attendre le build de l'image Docker (5-10 minutes)

2. **Attendre le chargement du mod√®le**
   - Le mod√®le Mistral 7B sera t√©l√©charg√© depuis Hugging Face (~10-15 minutes)
   - Surveiller les logs pour voir la progression
   - Message attendu : `‚úÖ Mod√®le Mistral 7B + LoRA charg√©!`

3. **R√©cup√©rer l'URL publique**
   - Dans le dashboard Vast.ai ‚Üí Votre instance ‚Üí **"Connect"** ou **"Public URL"**
   - L'URL sera de type : `http://votre-instance.vast.ai:8000` ou `https://votre-instance.vast.ai:8000`
   - Notez cette URL pour la configuration du frontend

---

## üß™ Tests des Endpoints

Une fois l'instance d√©marr√©e et le mod√®le charg√©, tester les endpoints :

### Test 1 : Health Check

```bash
curl http://votre-instance.vast.ai:8000/health
```

**R√©ponse attendue :**
```json
{
  "status": "ok",
  "model": "Mistral 7B + LoRA",
  "gpu_available": true
}
```

### Test 2 : Initialisation

```bash
curl http://votre-instance.vast.ai:8000/init
```

**R√©ponse attendue :**
```json
{
  "greeting": "Bonjour ! Je suis Spinoza. Discutons :\n\n**La libert√© est-elle une illusion ?**\n\nQu'en penses-tu ?",
  "history": [[null, "Bonjour ! Je suis Spinoza..."]]
}
```

### Test 3 : Chat

```bash
curl -X POST http://votre-instance.vast.ai:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Bonjour Spinoza, qu'\''est-ce que le conatus ?",
    "history": []
  }'
```

**R√©ponse attendue :**
```json
{
  "reply": "Le conatus est l'effort que chaque chose fait pour pers√©v√©rer dans son √™tre...",
  "history": [["Bonjour Spinoza...", "Le conatus est..."]]
}
```

### Test 4 : √âvaluation (Ma√Øeuthon)

```bash
curl -X POST http://votre-instance.vast.ai:8000/evaluate \
  -H "Content-Type: application/json" \
  -d '{
    "dialogue": "Spinoza: Bonjour ! Je suis Spinoza. Discutons : La libert√© est-elle une illusion ?\n√âl√®ve: Je pense que oui, tout est d√©termin√©.\nSpinoza: Tu dis que tout est d√©termin√©... qu'\''est-ce que √ßa veut dire pour toi ?",
    "score_front": 55
  }'
```

**R√©ponse attendue :**
```json
{
  "score_final": 85,
  "message_final": "Ton effort pour comprendre tes propres affects est impressionnant...",
  "details_model": {
    "comprehension": 8,
    "cooperation": 9,
    "progression": 8,
    "total": 25
  }
}
```

### Script de Test Automatique

Vous pouvez utiliser le script existant `test_runpod_deployment.sh` :

```bash
chmod +x Backend/test_runpod_deployment.sh
./Backend/test_runpod_deployment.sh http://votre-instance.vast.ai:8000
```

---

## üí∞ Co√ªts Estim√©s

### Par Heure
- **RTX 3090** : ~$0.20-0.40/h (~0.18-0.36‚Ç¨/h)
- **RTX 4090** : ~$0.40-0.60/h (~0.36-0.54‚Ç¨/h)

### Exemples de Co√ªts
- **3h de d√©mo** : ~$0.60-1.20 (0.54-1.08‚Ç¨)
- **8h/jour pendant 1 mois** : ~$48-96 (43-86‚Ç¨)
- **Usage ponctuel (d√©mos)** : Tr√®s √©conomique

### Optimisation des Co√ªts
- **Arr√™ter l'instance** imm√©diatement apr√®s usage
- **Ne pas laisser tourner** en veille
- **Utiliser un GPU moins puissant** (RTX 3090 au lieu de RTX 4090) si acceptable

---

## üîß Configuration Avanc√©e

### Variables d'Environnement Disponibles

| Variable | Description | Obligatoire | D√©faut |
|----------|-------------|-------------|--------|
| `HF_TOKEN` | Token Hugging Face pour t√©l√©charger le mod√®le | ‚úÖ Oui | - |
| `HUGGINGFACE_TOKEN` | Alias pour `HF_TOKEN` | ‚úÖ Oui (si `HF_TOKEN` absent) | - |
| `PORT` | Port FastAPI | ‚ùå Non | `8000` |

### Ports et R√©seau

- **Port interne** : 8000 (d√©fini dans le Dockerfile)
- **Port externe** : Mapp√© automatiquement par Vast.ai
- **URL publique** : G√©n√©r√©e automatiquement par Vast.ai

### Stockage Persistant (Optionnel)

Pour √©viter de ret√©l√©charger le mod√®le √† chaque d√©marrage :
- Utiliser un **Volume Disk persistant** dans Vast.ai
- Configurer le cache Hugging Face dans le volume
- **Co√ªt suppl√©mentaire** : ~$0.10-0.20/h pour le stockage

### Dockerfile avec CUDA Explicite (Optionnel)

Si vous rencontrez des probl√®mes de compatibilit√© CUDA, vous pouvez utiliser une image de base avec CUDA pr√©-install√© :

```dockerfile
FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

# Variables d'environnement
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

# Installer Python
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    build-essential \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# ... reste identique
```

**Note :** G√©n√©ralement non n√©cessaire, Vast.ai fournit CUDA dans l'environnement.

---

## üêõ Troubleshooting

### Le mod√®le ne charge pas

**Sympt√¥mes :** Erreur `ValueError: HF_TOKEN ou HUGGINGFACE_TOKEN doit √™tre d√©fini`

**Solutions :**
1. V√©rifier que `HF_TOKEN` est bien configur√© dans les variables d'environnement Vast.ai
2. V√©rifier que le token a les permissions de lecture sur Hugging Face
3. V√©rifier les logs de l'instance pour voir l'erreur exacte

### L'API ne r√©pond pas

**Sympt√¥mes :** Timeout ou erreur de connexion

**Solutions :**
1. V√©rifier que le port 8000 est bien expos√© dans la configuration Vast.ai
2. V√©rifier que l'instance est bien d√©marr√©e (status "Running")
3. V√©rifier les logs pour voir si le serveur FastAPI a d√©marr√©
4. Tester avec `curl` directement depuis votre machine

### Erreur de m√©moire (OOM)

**Sympt√¥mes :** `CUDA out of memory` dans les logs

**Solutions :**
1. Utiliser un GPU avec plus de VRAM (RTX 3090 24GB minimum)
2. R√©duire `max_new_tokens` dans `app_runpod.py` (lignes 401, 449, 489)
3. V√©rifier que la quantization 4-bit est bien activ√©e (d√©j√† fait dans le code)

### Le mod√®le est lent

**Sympt√¥mes :** Latence √©lev√©e (>10s par requ√™te)

**Solutions :**
1. V√©rifier que le GPU est bien utilis√© (`gpu_available: true` dans `/health`)
2. Utiliser un GPU plus puissant (RTX 4090 au lieu de RTX 3090)
3. Voir les optimisations de latence dans la documentation

### Probl√®me de compatibilit√© CUDA

**Sympt√¥mes :** Erreur `CUDA error` ou `bitsandbytes` ne fonctionne pas

**Solutions :**
1. **Vast.ai installe g√©n√©ralement CUDA automatiquement** - Le Dockerfile actuel devrait fonctionner
2. Si probl√®me persiste, v√©rifier la version CUDA dans les logs : `nvidia-smi`
3. V√©rifier que PyTorch d√©tecte le GPU : `torch.cuda.is_available()` dans les logs
4. **Compatibilit√© confirm√©e** : T4 et RTX 3090 sont tous deux support√©s par PyTorch 2.0+ et bitsandbytes 0.41.0+
5. Si n√©cessaire, utiliser une image Docker avec CUDA explicite (voir section Configuration Avanc√©e)

**Note :** Le code utilise des APIs g√©n√©riques (`torch.cuda.is_available()`, `device_map="auto"`) qui fonctionnent avec toutes les architectures NVIDIA r√©centes (Turing, Ampere, Ada).

---

## üìù Mise √† Jour du Frontend

Une fois l'URL Vast.ai obtenue, mettre √† jour le frontend :

1. Ouvrir `Frontend/index_spinoza.html`
2. Modifier la ligne 120 :
   ```javascript
   const API_BASE_URL = 'http://votre-instance.vast.ai:8000';
   ```
3. Tester la connexion compl√®te

**Voir le guide d√©taill√© :** `Frontend/GUIDE_UPDATE_VAST_AI.md`

---

## üîÑ Mise √† Jour et Maintenance

### Mettre √† Jour le Code

1. Modifier les fichiers localement
2. Pousser sur GitHub (si d√©ploiement depuis GitHub)
3. Red√©marrer l'instance Vast.ai pour rebuild

### Mettre √† Jour le Mod√®le

Le mod√®le sera ret√©l√©charg√© √† chaque d√©marrage si pas de volume persistant.

### Logs et Monitoring

- **Logs** : Accessibles dans le dashboard Vast.ai ‚Üí Votre instance ‚Üí Logs
- **Monitoring** : Utiliser `/health` pour v√©rifier l'√©tat
- **M√©triques** : Vast.ai fournit des m√©triques d'utilisation GPU

---

## üìö Ressources Compl√©mentaires

- **Documentation Vast.ai** : [docs.vast.ai](https://docs.vast.ai/)
- **Documentation Hugging Face** : [huggingface.co/docs](https://huggingface.co/docs)
- **Guide RunPod** (similaire) : `Backend/README_RUNPOD.md`
- **Architecture compl√®te** : `docs/references/ARCHITECTURE_COMPLETE.md`

---

## ‚úÖ Checklist de D√©ploiement

- [ ] Compte Vast.ai cr√©√©
- [ ] Token Hugging Face obtenu
- [ ] Instance Vast.ai cr√©√©e avec GPU appropri√©
- [ ] Variables d'environnement configur√©es (`HF_TOKEN`, `PORT`)
- [ ] Dockerfile configur√© (depuis GitHub ou direct)
- [ ] Instance d√©marr√©e et mod√®le charg√©
- [ ] URL publique r√©cup√©r√©e
- [ ] Tests des endpoints r√©ussis (`/health`, `/init`, `/chat`, `/evaluate`)
- [ ] Frontend mis √† jour avec la nouvelle URL
- [ ] Test complet frontend + backend r√©ussi

---

**Derni√®re mise √† jour :** D√©cembre 2024

