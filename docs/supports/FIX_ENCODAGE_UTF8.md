# üîß Fix Encodage UTF-8 - Probl√®me de caract√®res mal affich√©s

## Probl√®me

Les r√©ponses du mod√®le affichent des caract√®res mal encod√©s :
- `√É¬™tre` au lieu de `√™tre`
- `conna√É¬Ætre` au lieu de `conna√Ætre`
- `√É¬©volution` au lieu de `√©volution`
- `libert√É¬©` au lieu de `libert√©`

## Cause

Le probl√®me vient probablement du **tokenizer** qui d√©code les tokens en utilisant un mauvais encodage (latin-1 au lieu de UTF-8).

## Solution

### 1. Dans la fonction `spinoza_repond()` (notebook Colab)

Modifier la ligne de d√©codage pour forcer UTF-8 :

```python
# AVANT (ligne ~875)
response = tokenizer.decode(new_tokens, skip_special_tokens=True)

# APR√àS
response = tokenizer.decode(new_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)
# S'assurer que la r√©ponse est bien en UTF-8
if isinstance(response, bytes):
    response = response.decode('utf-8')
elif not isinstance(response, str):
    response = str(response)
```

### 2. V√©rifier l'encodage du tokenizer

Ajouter une v√©rification dans le chargement du mod√®le :

```python
# Apr√®s avoir charg√© le tokenizer
print(f"Tokenizer vocab size: {len(tokenizer)}")
print(f"Tokenizer encoding: {tokenizer.encoding if hasattr(tokenizer, 'encoding') else 'N/A'}")

# Tester le d√©codage
test_tokens = tokenizer.encode("√™tre libre")
test_decode = tokenizer.decode(test_tokens)
print(f"Test d√©codage: '{test_decode}'")
if "√™tre" not in test_decode:
    print("‚ö†Ô∏è PROBL√àME D'ENCODAGE D√âTECT√â!")
```

### 3. Forcer UTF-8 dans FastAPI (optionnel mais recommand√©)

Dans l'endpoint `/chat`, s'assurer que la r√©ponse est bien encod√©e :

```python
from fastapi import Response
from fastapi.responses import JSONResponse

@app.post("/chat")
def chat(req: ChatRequest):
    global conversation_history
    
    # Mettre √† jour historique si fourni
    if req.history:
        conversation_history = req.history
    
    # G√©n√©rer r√©ponse
    reply = spinoza_repond(req.message)
    
    # S'assurer que la r√©ponse est en UTF-8
    if isinstance(reply, bytes):
        reply = reply.decode('utf-8')
    
    # Nettoyer les caract√®res mal encod√©s (fallback)
    try:
        reply = reply.encode('latin-1').decode('utf-8')
    except (UnicodeDecodeError, UnicodeEncodeError):
        pass  # Si √ßa √©choue, garder la r√©ponse originale
    
    return JSONResponse(
        content={
            "reply": reply,
            "history": conversation_history
        },
        media_type="application/json; charset=utf-8"
    )
```

### 4. Solution alternative : Nettoyer la r√©ponse apr√®s d√©codage

Ajouter une fonction de nettoyage dans `spinoza_repond()` :

```python
def fix_encoding(text: str) -> str:
    """Corrige les probl√®mes d'encodage courants"""
    try:
        # Si le texte semble √™tre en latin-1 mal interpr√©t√©
        if '√É' in text or '√Ç' in text:
            # Essayer de corriger
            text = text.encode('latin-1').decode('utf-8')
    except (UnicodeDecodeError, UnicodeEncodeError):
        pass
    return text

# Dans spinoza_repond(), apr√®s le d√©codage :
response = tokenizer.decode(new_tokens, skip_special_tokens=True)
response = fix_encoding(response)  # Ajouter cette ligne
```

## V√©rification

Pour tester si le probl√®me est r√©solu :

1. **Tester avec une question contenant des accents** :
   ```
   "Qu'est-ce que la libert√© ?"
   ```

2. **V√©rifier la r√©ponse dans la console du navigateur** :
   ```javascript
   console.log(data.reply);
   // Devrait afficher : "Qu'est-ce que la libert√© ?" et non "Qu'est-ce que la libert√É¬© ?"
   ```

3. **V√©rifier les en-t√™tes HTTP** :
   ```bash
   curl -I https://votre-url.ngrok-free.dev/chat
   # Devrait contenir : Content-Type: application/json; charset=utf-8
   ```

## Solution rapide (temporaire)

Si vous ne pouvez pas modifier le notebook imm√©diatement, vous pouvez corriger c√¥t√© frontend dans `index_spinoza.html` :

```javascript
// Dans la fonction submitQuestion(), apr√®s avoir re√ßu data.reply
const data = await response.json();
let reply = data.reply;

// Corriger l'encodage si n√©cessaire
try {
    // Si le texte semble mal encod√©, essayer de le corriger
    if (reply.includes('√É') || reply.includes('√Ç')) {
        reply = reply.split('').map(char => {
            try {
                return char.charCodeAt(0) > 127 ? 
                    String.fromCharCode(char.charCodeAt(0)) : char;
            } catch {
                return char;
            }
        }).join('');
        // Essayer de r√©encoder
        reply = decodeURIComponent(escape(reply));
    }
} catch (e) {
    console.warn('Erreur correction encodage:', e);
}

// Utiliser reply corrig√©
conversationHistory.push([userMessage, reply]);
```

**Note :** Cette solution frontend est un contournement. La vraie solution est de corriger le probl√®me c√¥t√© backend dans le tokenizer.

