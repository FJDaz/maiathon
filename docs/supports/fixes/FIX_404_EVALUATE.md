# üîß Fix 404 sur /evaluate - Ajouter l'Endpoint Manquant

## ‚ùå Probl√®me

Le frontend appelle `/evaluate` mais cet endpoint n'existe pas dans votre notebook Colab, d'o√π le 404 :

```
nonremunerative-rory-unbreakably.ngrok-free.dev/evaluate:1  
Failed to load resource: the server responded with a status of 404 ()
```

## ‚úÖ Solution : Ajouter l'Endpoint /evaluate

### Option 1 : Version Simple (Recommand√©e pour d√©marrer)

Ajoutez cette cellule dans votre notebook Colab **apr√®s la cellule API FastAPI** (cellule 7) et **avant le lancement serveur** (cellule 8) :

```python
# =============================================================================
# üéÆ ENDPOINT /evaluate - √âvaluation Finale Ma√Øeuthon
# =============================================================================

from pydantic import BaseModel

class EvaluateRequest(BaseModel):
    dialogue: str
    score_front: int

class EvaluateResponse(BaseModel):
    score_final: int
    message_final: str
    details_model: dict

# ‚ö†Ô∏è PROMPT_EVALUATION (√† adapter selon vos besoins)
PROMPT_EVALUATION = """Tu √©values un dialogue entre Spinoza et un √©l√®ve.

DIALOGUE :

{dialogue}

GRILLE D'√âVALUATION (0-10 pour chaque crit√®re) :

1Ô∏è‚É£ COMPR√âHENSION :
   ‚Ä¢ 9-10 : Reformule correctement les id√©es, pose des questions pertinentes, fait des liens
   ‚Ä¢ 6-8 : Comprend mais demande des clarifications, fait quelques erreurs
   ‚Ä¢ 3-5 : Comprend partiellement, confusions fr√©quentes
   ‚Ä¢ 0-2 : Ne comprend pas, dit "je comprends pas", refuse le dialogue

2Ô∏è‚É£ COOP√âRATION :
   ‚Ä¢ 9-10 : R√©pond aux questions, fait avancer le dialogue, s'engage activement
   ‚Ä¢ 6-8 : Coop√®re mais avec quelques r√©sistances
   ‚Ä¢ 3-5 : R√©siste souvent, dialogue difficile
   ‚Ä¢ 0-2 : Refuse de coop√©rer, dit "j'ai autre chose √† faire", part

3Ô∏è‚É£ PROGRESSION :
   ‚Ä¢ 9-10 : Pens√©e √©volue clairement, conclusions √† la fin
   ‚Ä¢ 6-8 : Progresse lentement, quelques retours en arri√®re
   ‚Ä¢ 3-5 : Peu de progression, tourne en rond
   ‚Ä¢ 0-2 : Aucune √©volution, r√©p√®te les m√™mes incompr√©hensions

R√©ponds UNIQUEMENT avec ce JSON (aucun texte avant/apr√®s) :

{{
 "comprehension": X,
 "cooperation": Y,
 "progression": Z,
 "total": X+Y+Z
}}"""

# ‚ö†Ô∏è PROMPT_MESSAGE_FINAL (message de fin bienveillant)
PROMPT_MESSAGE_FINAL = """Tu es Spinoza. Tu viens de terminer un dialogue avec un √©l√®ve.

√âcris un message final court (2-3 phrases max) qui :
- Valide les efforts de l'√©l√®ve
- Encourage sa progression
- Reste chaleureux et motivant
- Ne soit jamais acad√©mique ou formel

Message :"""

def evaluer_dialogue(dialogue: str, score_front: int) -> dict:
    """
    √âvalue le dialogue complet et g√©n√®re le message final
    """
    import json
    import re
    
    # 1. √âvaluation (temp√©rature basse pour JSON strict)
    prompt_eval = PROMPT_EVALUATION.format(dialogue=dialogue)
    prompt_eval_formatted = f"<s>[INST] {prompt_eval} [/INST]"
    
    inputs = tokenizer(prompt_eval_formatted, return_tensors="pt").to(model.device)
    input_length = inputs['input_ids'].shape[1]
    
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.1,  # Basse temp√©rature pour JSON strict
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens = outputs[0][input_length:]
    reponse_eval = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    # Parser JSON
    details_model = None
    json_pattern = r'\{[^{}]*"comprehension"[^{}]*"cooperation"[^{}]*"progression"[^{}]*"total"[^{}]*\}'
    json_match = re.search(json_pattern, reponse_eval, re.DOTALL)
    
    if json_match:
        try:
            json_str = json_match.group(0).strip()
            details_model = json.loads(json_str)
        except json.JSONDecodeError:
            pass
    
    # Valeurs par d√©faut si parsing √©choue
    if not details_model or not isinstance(details_model, dict):
        details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
    
    # 2. Message final (temp√©rature plus haute pour cr√©ativit√©)
    prompt_final = PROMPT_MESSAGE_FINAL
    prompt_final_formatted = f"<s>[INST] {prompt_final} [/INST]"
    
    inputs_final = tokenizer(prompt_final_formatted, return_tensors="pt").to(model.device)
    input_length_final = inputs_final['input_ids'].shape[1]
    
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs_final = model.generate(
            **inputs_final,
            max_new_tokens=150,
            temperature=1.1,  # Haute temp√©rature pour cr√©ativit√©
            top_p=0.95,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens_final = outputs_final[0][input_length_final:]
    message_final = tokenizer.decode(new_tokens_final, skip_special_tokens=True).strip()
    
    # Nettoyer le message
    if message_final.startswith('"') and message_final.endswith('"'):
        message_final = message_final[1:-1]
    
    # Score total
    score_backend = details_model.get("total", 15)
    score_final = score_front + score_backend
    
    return {
        "score_final": score_final,
        "message_final": message_final,
        "details_model": details_model
    }

# Endpoint FastAPI
@app.post("/evaluate", response_model=EvaluateResponse)
def evaluate(req: EvaluateRequest):
    """
    √âvalue le dialogue complet et g√©n√®re le message final
    """
    result = evaluer_dialogue(req.dialogue, req.score_front)
    return EvaluateResponse(**result)

print("‚úÖ Endpoint /evaluate cr√©√© pour Ma√Øeuthon")
```

### Option 2 : Version Optimis√©e (Utilise les scores incr√©mentaux)

Si vous avez d√©j√† ajout√© l'√©valuation incr√©mentale (`/evaluate/incremental`), utilisez plut√¥t le fichier `ENDPOINT_EVALUATE_OPTIMISE.py` qui agr√®ge les scores incr√©mentaux.

## üìç O√π Placer dans Colab

1. **Ouvrez** votre notebook Colab
2. **Trouvez** la cellule 7 (API FastAPI) qui contient `@app.post("/chat")`
3. **Cr√©ez une nouvelle cellule** juste apr√®s
4. **Collez** le code ci-dessus
5. **Ex√©cutez** la cellule
6. **V√©rifiez** que vous voyez : `‚úÖ Endpoint /evaluate cr√©√© pour Ma√Øeuthon`

## ‚úÖ V√©rification

Apr√®s avoir ajout√© l'endpoint, testez dans la console du navigateur :

```javascript
// Tester l'endpoint
fetch('https://votre-url-ngrok.ngrok-free.dev/evaluate', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'ngrok-skip-browser-warning': 'true'
  },
  body: JSON.stringify({
    dialogue: "Test dialogue",
    score_front: 100
  })
})
.then(r => r.json())
.then(console.log)
```

Vous devriez recevoir une r√©ponse avec `score_final`, `message_final` et `details_model`.

## üîç Si √ßa ne fonctionne toujours pas

1. **V√©rifiez** que la cellule a bien √©t√© ex√©cut√©e (pas d'erreur)
2. **V√©rifiez** dans les logs Colab que l'endpoint est bien cr√©√©
3. **Red√©marrez** le serveur (relancez la cellule de lancement serveur)
4. **V√©rifiez** que l'URL ngrok est correcte dans le frontend

