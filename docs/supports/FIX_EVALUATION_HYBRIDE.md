# üîß Fix √âvaluation Hybride - Utiliser les Scores Incr√©mentaux

## Probl√®me actuel

L'√©valuation finale (`/evaluate`) fait **toujours** une √©valuation compl√®te, m√™me si des scores incr√©mentaux existent. Cela cr√©e une **double charge** sur le mod√®le :
- √âvaluation incr√©mentale (√©changes 2, 4) ‚Üí inutile si pas utilis√©e
- √âvaluation finale compl√®te (√©change 5) ‚Üí refait tout au lieu d'utiliser les scores incr√©mentaux

## Solution : √âvaluation Hybride

L'√©valuation finale doit **utiliser les scores incr√©mentaux** s'ils existent, et ne faire qu'une **√©valuation compl√®te** si les scores incr√©mentaux ne sont pas disponibles.

## Modification √† faire dans le Backend (Colab)

### Modifier l'endpoint `/evaluate` dans votre notebook Colab

**Remplacer la fonction `evaluer_dialogue()` ou l'endpoint `/evaluate`** par cette version optimis√©e :

```python
@app.post("/evaluate", response_model=EvaluateResponse)
def evaluate_endpoint(req: EvaluateRequest):
    """
    √âvaluation finale optimis√©e :
    - Si scores incr√©mentaux disponibles ‚Üí les agr√®ge + g√©n√®re seulement le message final
    - Sinon ‚Üí √©valuation compl√®te normale
    """
    dialogue_id = hash(req.dialogue)
    
    # V√©rifier si scores incr√©mentaux disponibles
    if dialogue_id in incremental_scores and len(incremental_scores[dialogue_id]) > 0:
        scores_inc = incremental_scores[dialogue_id]
        
        print(f"üìä Utilisation des scores incr√©mentaux ({len(scores_inc)} √©valuations)")
        
        # Agr√©ger les scores incr√©mentaux (moyenne pond√©r√©e par ordre)
        # Le dernier √©change est plus important (poids croissant)
        n = len(scores_inc)
        weights = [i+1 for i in range(n)]  # [1, 2, 3, ...]
        total_weight = sum(weights)
        
        # Calculer moyenne pond√©r√©e
        details_model = {
            "comprehension": int(round(
                sum(s["scores"]["comprehension"] * w for s, w in zip(scores_inc, weights)) / total_weight
            )),
            "cooperation": int(round(
                sum(s["scores"]["cooperation"] * w for s, w in zip(scores_inc, weights)) / total_weight
            )),
            "progression": int(round(
                sum(s["scores"]["progression"] * w for s, w in zip(scores_inc, weights)) / total_weight
            ))
        }
        details_model["total"] = sum([
            details_model["comprehension"],
            details_model["cooperation"],
            details_model["progression"]
        ])
        
        # Score total
        score_backend = details_model.get("total", 15)
        score_final = req.score_front + score_backend
        
        # ‚ö° OPTIMISATION : G√©n√©rer SEULEMENT le message final (scores d√©j√† calcul√©s)
        # Pas besoin de r√©√©valuer le dialogue complet
        prompt_final = PROMPT_MESSAGE_FINAL
        
        prompt_final_formatted = f"<s>[INST] {prompt_final} [/INST]"
        
        inputs_final = tokenizer(prompt_final_formatted, return_tensors="pt").to(model.device)
        input_length_final = inputs_final['input_ids'].shape[1]
        
        device_type = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
        
        with torch.autocast(device_type=device_type, dtype=dtype):
            outputs_final = model.generate(
                **inputs_final,
                max_new_tokens=150,
                temperature=1.1,  # Haute temp√©rature pour cr√©ativit√©
                top_p=0.95,
                do_sample=True,
                repetition_penalty=1.2,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        new_tokens_final = outputs_final[0][input_length_final:]
        message_final = tokenizer.decode(new_tokens_final, skip_special_tokens=True)
        message_final = message_final.strip()
        if message_final.startswith('"') and message_final.endswith('"'):
            message_final = message_final[1:-1]
        
        # Nettoyer les scores incr√©mentaux apr√®s utilisation (optionnel)
        # del incremental_scores[dialogue_id]
        
        return EvaluateResponse(
            score_final=score_final,
            message_final=message_final,
            details_model=details_model
        )
    
    else:
        # Fallback : √©valuation compl√®te si pas de scores incr√©mentaux
        print("‚ö†Ô∏è Pas de scores incr√©mentaux, √©valuation compl√®te")
        return evaluer_dialogue(req.dialogue, req.score_front)
```

## Avantages

### ‚úÖ Avant (syst√®me parall√®le)
- √âvaluation incr√©mentale : 2 appels mod√®le (√©changes 2, 4)
- √âvaluation finale : 2 appels mod√®le (scores + message)
- **Total : 4 appels mod√®le** pour un dialogue de 5 √©changes

### ‚úÖ Apr√®s (syst√®me hybride)
- √âvaluation incr√©mentale : 2 appels mod√®le (√©changes 2, 4) ‚Üí stock√©s
- √âvaluation finale : 1 appel mod√®le (message seulement, scores agr√©g√©s)
- **Total : 3 appels mod√®le** (gain de 25%)

## Impact sur la latence

- **√âvaluation finale** : R√©duite de ~50% (pas de r√©√©valuation compl√®te)
- **Charge mod√®le** : R√©duite de 25%
- **Qualit√©** : Maintenue (agr√©gation pond√©r√©e des scores incr√©mentaux)

## V√©rification

Apr√®s modification, v√©rifiez dans les logs Colab :
- `üìä Utilisation des scores incr√©mentaux (2 √©valuations)` ‚Üí syst√®me hybride actif
- `‚ö†Ô∏è Pas de scores incr√©mentaux, √©valuation compl√®te` ‚Üí fallback (normal si premier dialogue)

