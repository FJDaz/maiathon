# ‚úÖ Configuration Finale Vast.ai - Spinoza Secours

**Date :** 28 novembre 2025  
**Template :** NVIDIA CUDA  
**M√©thode :** On-start Script

---

## üìã Configuration √† Appliquer

### 1. Image Path:Tag

**Laisser tel quel :**
```
vastai/base-image:@vastai-automatic-tag
```

---

### 2. Ports

**Supprimer tous les ports existants et ajouter :**
```
8000
```

**Action :**
- Supprimer : `1111`, `6006`, `8080`, `8384`, `72299`
- Ajouter : `8000`

---

### 3. Environment Variables

**Supprimer toutes les variables existantes et ajouter :**

| Key | Value |
|-----|-------|
| `HF_TOKEN` | `votre_token_hf` ‚ö†Ô∏è **REMPLACER par votre vrai token** |
| `PORT` | `8000` |

**Variables √† supprimer :**
- `OPEN_BUTTON_PORT`
- `OPEN_BUTTON_TOKEN`
- `JUPYTER_DIR`
- `DATA_DIRECTORY`
- `PORTAL_CONFIG`

---

### 4. Docker Options

**Remplacer par :**
```
-p 8000:8000 -e HF_TOKEN=$HF_TOKEN -e PORT=8000
```

**Ou laisser vide si les variables d'environnement sont configur√©es s√©par√©ment.**

---

### 5. On-start Script

**Remplacer `entrypoint.sh` par :**

```bash
#!/bin/bash
set -e

echo "üöÄ D√©marrage Spinoza Secours sur Vast.ai..."

# Cr√©er r√©pertoire de travail
mkdir -p /workspace/spinoza-secours
cd /workspace/spinoza-secours

# Cloner le repository GitHub
echo "üì• Clonage du repository GitHub..."
if [ ! -d "maiathon" ]; then
    git clone https://github.com/FJDaz/maiathon.git
fi

cd maiathon/Spinoza_Secours_HF/Backend

# Installer les d√©pendances Python
echo "üì¶ Installation des d√©pendances..."
pip install --no-cache-dir --upgrade pip
pip install --no-cache-dir -r requirements.runpod.txt

# Lancer l'application FastAPI
echo "üöÄ Lancement de l'application FastAPI..."
python app_runpod.py
```

**‚ö†Ô∏è Important :** Copier-coller ce script dans le champ "On-start Script"

---

### 6. Disk Space

**Modifier de 32 GB √† :**
```
50 GB minimum (100 GB recommand√© pour Qwen 14B futur)
```

**Action :**
- Changer `32` ‚Üí `50` ou `100`

---

### 7. Launch Mode

**Choisir :**
```
Interactive shell server, SSH
```

**OU** laisser "Jupyter" si vous voulez garder Jupyter pour debug.

---

## ‚úÖ Checklist Avant "Create"

- [ ] **Ports :** 8000 uniquement (autres supprim√©s)
- [ ] **Environment Variables :**
  - [ ] `HF_TOKEN` = `votre_token_hf` ‚ö†Ô∏è **REMPLACER**
  - [ ] `PORT` = `8000`
  - [ ] Autres variables supprim√©es
- [ ] **Docker Options :** `-p 8000:8000 -e HF_TOKEN=$HF_TOKEN -e PORT=8000`
- [ ] **On-start Script :** Script ci-dessus copi√©
- [ ] **Disk Space :** 50-100 GB
- [ ] **Launch Mode :** Interactive shell (ou Jupyter si debug)

---

## üöÄ Apr√®s "Create"

### 1. Attendre le Build

**Temps estim√© :** 5-10 minutes

**Ce qui se passe :**
- Container d√©marre
- On-start Script s'ex√©cute
- Clone du repository GitHub
- Installation des d√©pendances Python
- Chargement du mod√®le Mistral 7B + LoRA (~10-15 min)

### 2. V√©rifier les Logs

**Dans le dashboard Vast.ai :**
- Aller dans votre instance
- Section "Logs" ou "Console"
- Chercher : `‚úÖ Mod√®le Mistral 7B + LoRA charg√©!`
- Chercher : `üöÄ D√©marrage du serveur FastAPI sur le port 8000...`

### 3. R√©cup√©rer l'URL Publique

**Dans le dashboard :**
- Instance ‚Üí "Connect" ou "Public URL"
- URL type : `http://votre-instance.vast.ai:8000`

### 4. Tester les Endpoints

```bash
# Health check
curl http://votre-instance.vast.ai:8000/health

# Init
curl http://votre-instance.vast.ai:8000/init

# Chat
curl -X POST http://votre-instance.vast.ai:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour", "history": []}'
```

---

## ‚ö†Ô∏è Points d'Attention

### 1. Token HF_TOKEN

**‚ö†Ô∏è CRITIQUE :** Remplacer `votre_token_hf` par votre vrai token Hugging Face.

**Format :** `hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`

**O√π l'obtenir :** https://huggingface.co/settings/tokens

### 2. Port 8000

**V√©rifier que :**
- Le port 8000 est bien expos√©
- Aucun autre service n'utilise le port 8000
- L'URL publique utilise le port 8000

### 3. Disk Space

**50 GB minimum n√©cessaire pour :**
- Mod√®le Mistral 7B : ~14GB
- LoRA adapter : ~100MB
- Syst√®me + d√©pendances : ~5GB
- Marge : ~30GB

**100 GB recommand√© pour :**
- Migration future vers Qwen 14B (~28GB)

### 4. On-start Script

**Le script s'ex√©cute √† chaque d√©marrage :**
- ‚úÖ Clone le repo (si pas d√©j√† pr√©sent)
- ‚úÖ Installe les d√©pendances
- ‚úÖ Lance l'application

**Temps d'ex√©cution :**
- Premi√®re fois : ~15-20 min (clone + install + mod√®le)
- Red√©marrages suivants : ~10-15 min (mod√®le ret√©l√©charg√© si Container Disk)

---

## üîß Troubleshooting

### Probl√®me : Script ne s'ex√©cute pas

**V√©rifier :**
- Les logs de l'instance
- Que le script est bien copi√© dans "On-start Script"
- Que les permissions sont correctes

### Probl√®me : Port 8000 non accessible

**V√©rifier :**
- Que le port 8000 est bien dans la liste des ports
- Que l'application FastAPI d√©marre (logs)
- Que l'URL publique est correcte

### Probl√®me : Mod√®le ne charge pas

**V√©rifier :**
- Que `HF_TOKEN` est correct
- Que le token a les permissions "read"
- Les logs pour voir les erreurs

---

## üìù R√©sum√© Configuration

```
Template: NVIDIA CUDA
Image: vastai/base-image:@vastai-automatic-tag
Ports: 8000
Environment Variables:
  - HF_TOKEN=votre_token_hf
  - PORT=8000
Docker Options: -p 8000:8000 -e HF_TOKEN=$HF_TOKEN -e PORT=8000
On-start Script: [Script ci-dessus]
Disk Space: 50-100 GB
Launch Mode: Interactive shell, SSH
```

---

## üîó R√©f√©rences

- **On-start Script :** `Backend/onstart_vast_ai.sh`
- **Dockerfile :** `Backend/Dockerfile.runpod`
- **Repository :** https://github.com/FJDaz/maiathon
- **Plan migration :** `docs/references/PLAN_MIGRATION_VAST_AI.md`

---

**‚úÖ Une fois la configuration termin√©e, cliquer "Create" et attendre le d√©ploiement !**

