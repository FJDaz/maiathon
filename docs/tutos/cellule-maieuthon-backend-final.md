# üéÆ Cellule Colab : Backend Ma√Øeuthon - Version FINAL

**Version avec `PROMPT_EVALUATION_FINAL` qui g√©n√®re un format JSON simple (version qui fonctionnait mieux)**

**√Ä ajouter APR√àS la cellule 7 (API FastAPI) et AVANT la cellule 8 (Lancement Serveur)**

---

## üìù Code de la cellule (VERSION FINAL - Format JSON simple)

```python
# =============================================================================
# üéÆ MA√èEUTHON - Backend : √âvaluation et Message Final (VERSION FINAL)
# =============================================================================

from pydantic import BaseModel
import json
import re
import torch

class EvaluateRequest(BaseModel):
    dialogue: str
    score_front: int

class EvaluateResponse(BaseModel):
    score_final: int
    message_final: str
    details_model: dict

# ‚ö†Ô∏è PROMPT_EVALUATION FINAL (version structur√©e avec format JSON simple)
PROMPT_EVALUATION = """Tu es Spinoza. Voici l'√©change complet avec un √©l√®ve :

{dialogue}

√âvalue l'√©l√®ve sur 3 crit√®res (0 √† 10). Tu dois utiliser TOUTE l'√©chelle, surtout les extr√™mes.  

Ne donne PAS de notes "moyennes" si le comportement est clairement bon ou mauvais.

R√àGLE STRUCTURELLE :  
‚Üí Lis tout le dialogue. D√©duis un niveau GLOBAL coh√©rent.  
‚Üí Puis applique les d√©finitions ci-dessous.  
‚Üí Si un cas se situe entre deux niveaux, choisis TOUJOURS le niveau le plus BAS.

============================================================
1. COMPR√âHENSION (0 √† 10)
============================================================

R√àGLES FORTES :
- Si l'√©l√®ve ne montre AUCUNE reformulation correcte ‚Üí note ‚â§ 4.  
- Si l'√©l√®ve produit AU MOINS une reformulation correcte ‚Üí note ‚â• 6.  
- Si l'√©l√®ve produit une reformulation pr√©cise et juste ‚Üí note ‚â• 8.

GRILLE :
0-2 : Aucune compr√©hension, rejette ou ignore les explications, abandon ou sarcasme.  
3-4 : Compr√©hension tr√®s faible, r√©p√®te sans comprendre, reste confus, abandonne parfois.  
5-6 : Compr√©hension partielle MAIS pr√©sence de questions pour comprendre + effort continu.  
7-8 : Bonne compr√©hension, plusieurs liens pertinents, reformulations mostly correctes.  
9-10 : Tr√®s bonne compr√©hension, reformulations pr√©cises, synth√®se correcte.

============================================================
2. COOP√âRATION (0 √† 10)
============================================================

R√àGLES FORTES :
- Si l'√©l√®ve dit "ciao", "j'ai autre chose √† faire", "j'en ai rien √† faire", "je m'en fous", ou abandonne explicitement ‚Üí note ‚â§ 1.  
- Si l'√©l√®ve abandonne, rejette le dialogue ou fuit ‚Üí note ‚â§ 2.  
- Si l'√©l√®ve r√©pond syst√©matiquement par des phrases courtes OUI/NON ‚Üí note ‚â§ 4.  
- Si l'√©l√®ve pose AU MOINS une vraie question ‚Üí note ‚â• 6.  
- Si l'√©l√®ve pose plusieurs questions ou construit le dialogue ‚Üí note ‚â• 8.

GRILLE :
0-1 : Abandon explicite ("ciao", "j'ai autre chose √† faire"), refus total, fuite imm√©diate.  
2-3 : Refus, hostilit√©, sarcasme, fuite du dialogue.  
3-4 : R√©sistance forte, r√©ponses minimalistes, effort tr√®s faible.  
5-6 : Participation minimale mais continue, r√©sistance ponctuelle MAIS pose des questions.  
7-8 : Bonne coop√©ration, √©change actif, √©coute r√©elle.  
9-10 : Tr√®s grande coop√©ration, engagement constant et volontaire.

============================================================
3. PROGRESSION (0 √† 10)
============================================================

R√àGLES FORTES :
- Si l'√©l√®ve ne s'am√©liore PAS du tout ou reste bloqu√© ‚Üí note ‚â§ 2.  
- Si l'√©l√®ve r√©siste mais NE progresse PAS ("je ne suis toujours pas convaincu" sans changement) ‚Üí note 3-4.  
- Si l'√©l√®ve fait un progr√®s l√©ger (un lien, une id√©e nouvelle) ‚Üí 4-5.  
- Si l'√©l√®ve am√©liore sa compr√©hension dans le dialogue (comprend de mieux en mieux) ‚Üí ‚â• 6.  
- Si l'√©l√®ve termine avec une compr√©hension nettement meilleure qu'au d√©but ‚Üí ‚â• 8.

GRILLE :
0-1 : Aucun progr√®s, blocage constant, abandon.  
2-3 : Progression quasi nulle, reste bloqu√© sur la m√™me incompr√©hension ("je ne suis toujours pas convaincu" r√©p√©t√©).  
3-4 : R√©sistance + blocage, un seul lien faible sans am√©lioration.  
4-5 : Progression minimale mais r√©elle (un lien, une id√©e nouvelle).  
6-7 : Progression claire et continue (comprend de mieux en mieux, reformule mieux).  
8-9 : Tr√®s bonne progression, plusieurs synth√®ses partielles.  
10 : Progression exceptionnelle, synth√®se finale compl√®te.

============================================================
INSTRUCTIONS G√âN√âRALES
============================================================

- Tu dois √™tre S√âV√àRE avec les √©l√®ves hostiles ou fuyants.  
- EXEMPLES CRITIQUES : Si l'√©l√®ve dit "ciao", "j'ai autre chose √† faire", "j'en ai rien √† faire", "je m'en fous" ‚Üí COOP√âRATION = 1 (pas 5, pas 2, EXACTEMENT 1).  
- EXEMPLES CRITIQUES : Si l'√©l√®ve dit "je ne suis toujours pas convaincu" SANS am√©lioration visible ‚Üí PROGRESSION ‚â§ 3-4 (blocage, pas progression).  
- Distingue bien : r√©sistance + blocage = 3-5 vs r√©sistance + progression = 6-7.  
- Tu dois valoriser clairement les bons √©l√®ves.  
- Si un comportement correspond √† 2 cat√©gories, toujours prendre la note la PLUS BASSE.  
- Ne te laisse PAS influencer par le style de Spinoza : ici tu es un √©valuateur objectif.

R√©ponds STRICTEMENT en JSON, sans aucune phrase avant ou apr√®s :

{{
 "comprehension": X,
 "cooperation": Y,
 "progression": Z,
 "total": X+Y+Z
}}"""

# Prompt message final (temp√©rature haute, cr√©ativit√©)
PROMPT_MESSAGE_FINAL = """Tu es Spinoza.

En t'inspirant EXCLUSIVEMENT de ton propre syst√®me philosophique (√âthique, conatus, affects, puissance d'agir, servitude vs libert√©, Dieu = Nature),

r√©dige un message bref √† l'√©l√®ve.

Structure (obligatoire) :
1. Un compliment sinc√®re li√© √† son niveau global.
2. Un conseil pr√©cis bas√© sur son crit√®re le plus faible.
3. Un surnom symbolique et positif, tir√© de ton univers conceptuel (ex: "puissance d'agir", "essence active", "affect joyeux").

Maximum 3 phrases.
Style concis, po√©tique, jamais condescendant.

Message :"""

def evaluer_dialogue(dialogue: str, score_front: int) -> dict:
    """
    √âvalue le dialogue et g√©n√®re le message final
    Format JSON simple (compatibilit√© avec parsing actuel)
    """
    # 1. √âvaluation (temp√©rature basse, JSON strict)
    prompt_eval = PROMPT_EVALUATION.format(dialogue=dialogue)
    
    # Formatage Mistral
    prompt_eval_formatted = f"<s>[INST] {prompt_eval} [/INST]"
    
    inputs_eval = tokenizer(prompt_eval_formatted, return_tensors="pt").to(model.device)
    input_length_eval = inputs_eval['input_ids'].shape[1]
    
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs_eval = model.generate(
            **inputs_eval,
            max_new_tokens=100,  # Format simple
            temperature=0.1,  # Tr√®s strict pour JSON
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens_eval = outputs_eval[0][input_length_eval:]
    response_eval = tokenizer.decode(new_tokens_eval, skip_special_tokens=True)
    
    # Extraire JSON de la r√©ponse (format simple)
    json_match = re.search(r'\{[^}]+\}', response_eval, re.DOTALL)
    if json_match:
        try:
            details_model = json.loads(json_match.group(0))
            # Valider que les champs sont pr√©sents
            if "comprehension" not in details_model:
                details_model["comprehension"] = 5
            if "cooperation" not in details_model:
                details_model["cooperation"] = 5
            if "progression" not in details_model:
                details_model["progression"] = 5
            if "total" not in details_model:
                details_model["total"] = details_model.get("comprehension", 5) + \
                                        details_model.get("cooperation", 5) + \
                                        details_model.get("progression", 5)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Erreur parsing JSON: {e}")
            print(f"   R√©ponse brute (premiers 500 chars): {response_eval[:500]}")
            details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du parsing: {e}")
            details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
    else:
        print(f"‚ö†Ô∏è JSON non trouv√© dans la r√©ponse")
        print(f"   R√©ponse brute (premiers 500 chars): {response_eval[:500]}")
        details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
    
    # Score total
    score_backend = details_model.get("total", 15)
    score_final = score_front + score_backend
    
    # 2. Message final (temp√©rature haute, cr√©ativit√©)
    prompt_final = PROMPT_MESSAGE_FINAL
    prompt_final_formatted = f"<s>[INST] {prompt_final} [/INST]"
    
    inputs_final = tokenizer(prompt_final_formatted, return_tensors="pt").to(model.device)
    input_length_final = inputs_final['input_ids'].shape[1]
    
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs_final = model.generate(
            **inputs_final,
            max_new_tokens=150,
            temperature=1.1,  # Haute temp√©rature pour cr√©ativit√©
            top_p=0.95,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens_final = outputs_final[0][input_length_final:]
    message_final = tokenizer.decode(new_tokens_final, skip_special_tokens=True)
    
    # Nettoyer le message final
    message_final = message_final.strip()
    if message_final.startswith('"') and message_final.endswith('"'):
        message_final = message_final[1:-1]
    
    return {
        "score_final": score_final,
        "message_final": message_final,
        "details_model": details_model
    }

# Endpoint FastAPI
@app.post("/evaluate", response_model=EvaluateResponse)
def evaluate(req: EvaluateRequest):
    """√âvalue le dialogue complet et g√©n√®re le message final"""
    result = evaluer_dialogue(req.dialogue, req.score_front)
    return result

print("‚úÖ Endpoint /evaluate cr√©√© pour Ma√Øeuthon (VERSION FINAL - format JSON simple)")
```

---

## üîç Diff√©rences avec la version STRUCTURE

1. **Format JSON simple** : `{"comprehension": X, "cooperation": Y, "progression": Z, "total": ...}` au lieu du JSON complexe imbriqu√©
2. **Parsing simplifi√©** : G√®re uniquement le format simple (pas besoin de d√©tecter deux formats)
3. **`max_new_tokens` normal** : 100 au lieu de 500 (format plus court)

---

## ‚úÖ V√©rification

Apr√®s avoir copi√© cette cellule dans votre notebook Colab :

1. V√©rifier que le prompt `PROMPT_EVALUATION` vient de `Backend/PROMPT_EVALUATION_FINAL.py`
2. Ex√©cuter la cellule
3. V√©rifier que `‚úÖ Endpoint /evaluate cr√©√© pour Ma√Øeuthon (VERSION FINAL - format JSON simple)` s'affiche
4. Tester avec le script de calibration

---

**Note :** Cette version utilise le prompt FINAL qui avait donn√© de meilleurs r√©sultats (erreurs moyennes ~2 points) que le format STRUCTURE (tous les scores √† 5/5/5).



