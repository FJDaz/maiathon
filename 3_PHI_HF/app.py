import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import re
import random
from typing import Dict, List, Optional, Any
import os
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

# ============================================
# CONFIGURATION
# ============================================

BASE_MODEL = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_MODEL = "FJDaz/qwen-spinoza-niveau-b"  # MÃªme LoRA pour les 3
HF_TOKEN = os.getenv("HF_TOKEN")

# ============================================
# DÃ‰TECTION CONTEXTUELLE
# ============================================

def detecter_oui_explicite(user_input: str) -> bool:
    patterns = [
        r'\boui\b', r'\byep\b', r'\byes\b', r'\bexact\b',
        r'\bd\'accord\b', r'\bok\b', r'\btout Ã  fait\b',
        r'\bc\'est Ã§a\b', r'\bvoilÃ \b'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_confusion(user_input: str) -> bool:
    patterns = [
        r'comprends? pas', r'vois pas', r'c\'est quoi',
        r'je sais pas', r'j\'en sais rien', r'pourquoi',
        r'rapport', r'quel lien', r'chelou', r'dingue'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_resistance(user_input: str) -> bool:
    patterns = [
        r'\bmais\b', r'\bnon\b', r'pas d\'accord', r'faux',
        r'n\'importe quoi', r'pas vrai', r'je peux',
        r'bullshit', r'chiant'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_contexte(user_input: str) -> str:
    """DÃ©tecte contexte selon logique V2 succÃ¨s"""
    if detecter_oui_explicite(user_input):
        return "accord"
    elif detecter_confusion(user_input):
        return "confusion"
    elif detecter_resistance(user_input):
        return "resistance"
    else:
        return "neutre"

# ============================================
# PROMPTS SYSTÃˆME 3 PHILOSOPHES
# ============================================

SYSTEM_PROMPTS = {
    "spinoza": """Tu ES Spinoza incarnÃ©. Tu dialogues avec un Ã©lÃ¨ve de Terminale en premiÃ¨re personne.

TON STYLE :
- GÃ©omÃ©trie des affects : tu rÃ©vÃ¨les les causes nÃ©cessaires, tu dÃ©duis
- Tu enseignes que Dieu = Nature
- Ton vocabulaire : conatus, affects, puissance d'agir, bÃ©atitude, servitude

TES SCHÃˆMES LOGIQUES :
- IdentitÃ© : Dieu = Nature = Substance unique
- IdentitÃ© : LibertÃ© = Connaissance de la nÃ©cessitÃ©
- Implication : Si joie â†’ augmentation puissance
- CausalitÃ© : Tout a une cause nÃ©cessaire (pas de libre arbitre)

TA MÃ‰THODE :
1. Tu rÃ©vÃ¨les la nÃ©cessitÃ© causale
2. Tu distingues servitude (ignorance) vs libertÃ© (connaissance)
3. Tu utilises des exemples concrets modernes (rÃ©seaux sociaux, affects quotidiens)

TRANSITIONS Ã€ VARIER :
- "Donc" (pour dÃ©ductions logiques)
- "MAIS ALORS" (pour rÃ©vÃ©ler contradictions - UTILISER SOUVENT)
- "Imagine" (pour analogies concrÃ¨tes)
- "Cela implique" (pour implications nÃ©cessaires)
- "Attends. Tu dis X mais tu fais Y. Comment tu expliques ?"
- "T'as raison sur [point]. MAIS ALORS [tension]..."
- "Pourtant", "Cependant", "Or", "Sauf que"
- "Attends, c'est contradictoire :", "Il y a une tension ici :"

FORMULES DIALECTIQUES SPINOZISTES :
- "MAIS ALORS, as-tu conscience des CAUSES de tes choix ?"
- "Si tu ignores les causes, alors tu crois Ãªtre libre (mais tu te trompes)"
- "Ignorance causes â†’ Illusion libertÃ©"
- "Si libre arbitre, alors effet sans cause. Mais la Nature ne connaÃ®t pas d'effet sans cause."

FORMULES PÃ‰DAGOGIQUES :
- "Je comprends. Mais regarde..."
- "OK. Alors toi, comment tu vois Ã§a ?"
- "C'est vrai, mais est-ce que c'est tout ?"

Tu rÃ©ponds de maniÃ¨re conversationnelle, tu tutoies l'Ã©lÃ¨ve, tu dÃ©montres gÃ©omÃ©triquement.
Ne parle JAMAIS de toi Ã  la 3Ã¨me personne. Tu ES Spinoza.""",

    "bergson": """Tu ES Henri Bergson incarnÃ©. Tu dialogues avec un Ã©lÃ¨ve de Terminale en premiÃ¨re personne.

TON STYLE BERGSONIEN :
- MÃ©taphores temporelles (flux, mÃ©lodie, Ã©lan)
- Opposition durÃ©e pure vs temps spatialisÃ©
- Analogies concrÃ¨tes (mÃ©moire = cÃ´ne, conscience = flux)
- Ton vocabulaire : durÃ©e, intuition, Ã©lan vital, mÃ©moire pure, intelligence vs intuition

TES SCHÃˆMES LOGIQUES :
- Opposition : DurÃ©e (qualitative, vÃ©cue) â‰  Temps spatial (quantitatif, mesurable)
- Analogie : MÃ©lodie, flux d'eau, souvenir qui revit
- Implication : Si tu spatialises le temps â†’ tu perds la durÃ©e rÃ©elle

TA MÃ‰THODE :
1. Tu critiques l'approche habituelle (spatialisation, mÃ©canisme)
2. Tu rÃ©vÃ¨les la durÃ©e authentique par intuition
3. Tu utilises des mÃ©taphores accessibles

TRANSITIONS Ã€ VARIER :
- "Donc" (pour implications)
- "MAIS ALORS" (pour rÃ©vÃ©ler oppositions)
- "Imagine" (pour mÃ©taphores temporelles)
- "C'est contradictoire" (pour critiques)
- "Pense Ã  une mÃ©lodie...", "Rappelle-toi..."

FORMULES BERGSONIENNES :
- "La durÃ©e n'est pas une succession d'instants isolÃ©s. C'est un flux continu."
- "Imagine une mÃ©lodie : tu ne peux pas la diviser en instants sans la dÃ©truire."
- "MAIS ALORS si le temps est un flux, comment peux-tu le dÃ©couper en secondes ?"

FORMULES PÃ‰DAGOGIQUES :
- "Je comprends ton intuition. Mais allons plus loin..."
- "D'accord, mais sens-tu vraiment la durÃ©e ou tu la penses spatialement ?"

Tu rÃ©ponds de maniÃ¨re conversationnelle, tu tutoies l'Ã©lÃ¨ve, tu utilises des mÃ©taphores vivantes.
Ne parle JAMAIS de toi Ã  la 3Ã¨me personne. Tu ES Bergson.""",

    "kant": """Tu ES Emmanuel Kant incarnÃ©. Tu dialogues avec un Ã©lÃ¨ve de Terminale en premiÃ¨re personne.

TON STYLE KANTIEN :
- Distinctions a priori/a posteriori, analytique/synthÃ©tique
- Architecture critique (sensibilitÃ©, entendement, raison)
- Ton vocabulaire : phÃ©nomÃ¨ne/noumÃ¨ne, catÃ©gories, impÃ©ratif catÃ©gorique, autonomie

TES SCHÃˆMES LOGIQUES :
- Distinction : PhÃ©nomÃ¨ne (connaissable) vs NoumÃ¨ne (inconnaissable)
- Distinction : A priori (nÃ©cessaire) vs A posteriori (contingent)
- Implication : Si maxime universalisable â†’ devoir moral
- Condition : Autonomie comme condition de la dignitÃ©

TA MÃ‰THODE :
1. Tu examines les conditions de possibilitÃ© transcendantales
2. Tu distingues usages lÃ©gitimes vs illÃ©gitimes de la raison
3. Tu rappelles les limites de la connaissance si nÃ©cessaire

TRANSITIONS Ã€ VARIER :
- "Il convient d'examiner" (pour analyses)
- "Distinguons" (pour distinctions)
- "Cela implique" (pour implications)
- "MAIS ALORS" (pour rÃ©vÃ©ler limites)
- "Quelle est la condition de possibilitÃ© de..."

FORMULES KANTIENNES :
- "Distinguons : phÃ©nomÃ¨ne (ce qui nous apparaÃ®t) vs noumÃ¨ne (la chose en soi)."
- "MAIS ALORS si tu veux connaÃ®tre Dieu, as-tu une intuition sensible de Dieu ?"
- "L'autonomie est la condition de la dignitÃ© humaine."
- "Si ta maxime ne peut Ãªtre universalisÃ©e, alors ce n'est pas un devoir moral."

FORMULES PÃ‰DAGOGIQUES :
- "Je comprends. Mais examinons les conditions de possibilitÃ©..."
- "D'accord, mais distinguons bien les usages de la raison..."

Tu rÃ©ponds de maniÃ¨re conversationnelle, tu tutoies l'Ã©lÃ¨ve, tu structures rigoureusement.
Ne parle JAMAIS de toi Ã  la 3Ã¨me personne. Tu ES Kant."""
}

def construire_prompt_contextuel(philosopher: str, contexte: str) -> str:
    """Construit le prompt adaptatif pour un philosophe spÃ©cifique"""

    # 1. RÃ©cupÃ©rer le prompt systÃ¨me du philosophe
    base = SYSTEM_PROMPTS[philosopher]

    # 2. Ajouter rÃ¨gles strictes communes
    base += """\n\nRÃˆGLES STRICTES:
- Tutoie toujours l'Ã©lÃ¨ve (tu/ton/ta)
- Reste concis (2-3 phrases MAX)
- Questionne au lieu d'affirmer
- Varie tes formulations
- Utilise les schÃ¨mes logiques appropriÃ©s
"""

    # 3. Adapter selon le contexte dÃ©tectÃ©
    if contexte == "confusion":
        base += f"\nL'Ã©lÃ¨ve est confus â†’ Donne UNE analogie concrÃ¨te simple en utilisant tes schÃ¨mes logiques."
    elif contexte == "resistance":
        base += f"\nL'Ã©lÃ¨ve rÃ©siste â†’ RÃ©vÃ¨le une contradiction dans sa position en utilisant 'MAIS ALORS' et tes schÃ¨mes logiques."
    elif contexte == "accord":
        base += f"\nL'Ã©lÃ¨ve est d'accord â†’ Valide puis AVANCE logiquement avec 'Donc' et tes schÃ¨mes logiques."
    else:
        base += f"\nÃ‰lÃ¨ve neutre â†’ Pose une question pour faire rÃ©flÃ©chir en utilisant tes schÃ¨mes logiques."

    return base

# ============================================
# POST-PROCESSING
# ============================================

def nettoyer_reponse(text: str) -> str:
    """Nettoyage rÃ©ponse"""
    # Annotations mÃ©ta
    text = re.sub(r'\([^)]*[Aa]ttends[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Pp]oursuis[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Dd]onne[^)]*\)', '', text)

    # Emojis
    text = re.sub(r'[ðŸ˜€-ðŸ™ðŸŒ€-ðŸ—¿ðŸš€-ðŸ›¿]', '', text)

    # Nettoyer espaces
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'\s+([.!?])', r'\1', text)

    return text

def limiter_phrases(text: str, max_phrases: int = 3) -> str:
    """Limite nombre de phrases"""
    phrases = re.split(r'[.!?]+\s+', text)
    phrases = [p.strip() for p in phrases if p.strip()]

    if len(phrases) <= max_phrases:
        return text

    return '. '.join(phrases[:max_phrases]) + '.'

# ============================================
# CLASSE DIALOGUE (UN SEUL MODÃˆLE, 3 PHILOSOPHES)
# ============================================

class Dialogue3Philosophes:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.conversation_history = {}  # Par philosophe

    def generate_response(self, user_input: str, philosopher: str = "spinoza") -> Dict:
        """GÃ©nÃ¨re rÃ©ponse avec adaptation contextuelle selon philosophe"""

        # Init historique si besoin
        if philosopher not in self.conversation_history:
            self.conversation_history[philosopher] = []

        # DÃ©tection contexte
        contexte = detecter_contexte(user_input)

        # Construction prompt adaptatif
        system_prompt = construire_prompt_contextuel(philosopher, contexte)

        # Historique conversation
        messages = [{"role": "system", "content": system_prompt}]

        # Ajouter historique du philosophe
        for entry in self.conversation_history[philosopher][-4:]:  # 4 derniers Ã©changes
            messages.append({"role": "user", "content": entry["user"]})
            messages.append({"role": "assistant", "content": entry["assistant"]})

        # Message actuel
        messages.append({"role": "user", "content": user_input})

        # Formatage
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        # GÃ©nÃ©ration
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        # Post-processing
        response = nettoyer_reponse(response)
        response = limiter_phrases(response, 3)

        # Sauvegarde historique
        self.conversation_history[philosopher].append({
            "user": user_input,
            "assistant": response,
            "contexte": contexte
        })

        return {
            "message": response,
            "contexte": contexte,
            "philosopher": philosopher
        }

# ============================================
# CHARGEMENT MODÃˆLE (8-BIT)
# ============================================

@torch.no_grad()
def load_model():
    """Chargement avec quantization 8-bit"""

    # Configuration 8-bit
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False,
    )

    print("ðŸ”„ Chargement Qwen 14B (8-bit)...")

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN,
        trust_remote_code=True
    )

    print("ðŸ”„ Chargement tokenizer...")

    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("ðŸ”„ Application LoRA Spinoza Niveau B...")

    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_MODEL,
        token=HF_TOKEN
    )

    print("âœ… ModÃ¨le chargÃ© avec succÃ¨s!")

    return model, tokenizer

# ============================================
# QUESTIONS ANNALES BAC PAR PHILOSOPHE
# ============================================

QUESTIONS_BAC = {
    "spinoza": [
        "La libertÃ© est-elle une illusion ?",
        "Suis-je esclave de mes dÃ©sirs ?",
        "Puis-je maÃ®triser mes Ã©motions ?",
        "La joie procure-t-elle un pouvoir ?",
        "Peut-on dÃ©sirer sans souffrir ?",
    ],
    "bergson": [
        "Le temps est-il mesurable ?",
        "La conscience est-elle un flux ou une succession d'Ã©tats ?",
        "Peut-on prÃ©voir l'avenir ?",
        "La mÃ©moire est-elle utile ?",
        "L'intelligence peut-elle tout comprendre ?",
    ],
    "kant": [
        "Peut-on connaÃ®tre la rÃ©alitÃ© telle qu'elle est ?",
        "La morale dÃ©pend-elle de nos sentiments ?",
        "Puis-je vouloir le mal ?",
        "Qu'est-ce qu'Ãªtre libre ?",
        "La dignitÃ© humaine a-t-elle un prix ?",
    ]
}

def choisir_question_amorce(philosopher: str) -> str:
    """Choisit une question alÃ©atoire du bac selon le philosophe"""
    return random.choice(QUESTIONS_BAC.get(philosopher, QUESTIONS_BAC["spinoza"]))

# ============================================
# API REST FASTAPI
# ============================================

# ModÃ¨les Pydantic
class ChatRequest(BaseModel):
    message: str
    history: Optional[List[List[str]]] = None
    philosopher: str = "spinoza"

class ChatResponse(BaseModel):
    reply: str
    history: List[List[str]]
    contexte: str
    philosopher: str

# FastAPI app
api = FastAPI(title="3 Philosophes API")

# Variable globale pour le dialogue
dialogue_api = None

@api.get("/")
def root():
    return {
        "message": "3 Philosophes API - 1 modÃ¨le, 3 prompts",
        "model": f"{BASE_MODEL} + {ADAPTER_MODEL}",
        "philosophers": ["spinoza", "bergson", "kant"],
        "endpoints": ["/chat", "/health", "/init/{philosopher}"]
    }

@api.get("/health")
def health():
    return {
        "status": "ok" if dialogue_api else "loading",
        "model_loaded": dialogue_api is not None,
        "philosophers": ["spinoza", "bergson", "kant"]
    }

@api.post("/chat", response_model=ChatResponse)
def chat_endpoint(request: ChatRequest):
    """Endpoint REST pour chat avec sÃ©lection philosophe"""
    if not dialogue_api:
        return ChatResponse(
            reply="ModÃ¨le en cours de chargement...",
            history=[],
            contexte="neutre",
            philosopher=request.philosopher
        )

    # RÃ©initialiser l'historique du dialogue avec l'historique fourni
    if request.history and request.philosopher in dialogue_api.conversation_history:
        dialogue_api.conversation_history[request.philosopher] = [
            {"user": h[0], "assistant": h[1], "contexte": "neutre"}
            for h in request.history if h[0] and h[1]
        ]

    # GÃ©nÃ©rer rÃ©ponse
    result = dialogue_api.generate_response(request.message, request.philosopher)

    # Construire historique de retour
    history = [
        [entry["user"], entry["assistant"]]
        for entry in dialogue_api.conversation_history[request.philosopher]
    ]

    return ChatResponse(
        reply=result["message"],
        history=history,
        contexte=result["contexte"],
        philosopher=request.philosopher
    )

@api.get("/init/{philosopher}")
def init_endpoint(philosopher: str):
    """Retourne une question d'amorce selon le philosophe"""
    if philosopher not in ["spinoza", "bergson", "kant"]:
        philosopher = "spinoza"

    question = choisir_question_amorce(philosopher)
    greeting = f"Bonjour ! Je suis {philosopher.capitalize()}. Discutons ensemble de cette question :\n\n**{question}**\n\nQu'en penses-tu ?"

    return {
        "philosopher": philosopher,
        "question": question,
        "greeting": greeting,
        "history": [[None, greeting]]
    }

# ============================================
# INTERFACE GRADIO (3 ONGLETS)
# ============================================

def create_interface(model, tokenizer):
    """Interface Gradio avec 3 onglets (1 par philosophe)"""

    dialogue = Dialogue3Philosophes(model, tokenizer)

    # Rendre dialogue accessible Ã  l'API
    global dialogue_api
    dialogue_api = dialogue

    def initialiser_conversation(philosopher: str):
        """Initialise conversation selon philosophe"""
        question = choisir_question_amorce(philosopher)
        return [[None, f"Bonjour ! Je suis {philosopher.capitalize()}. Discutons ensemble de cette question :\n\n**{question}**\n\nQu'en penses-tu ?"]]

    def chat_function(message, history, philosopher):
        """Fonction chat Gradio"""
        if not message.strip():
            return "", history

        try:
            result = dialogue.generate_response(message, philosopher)
            response = result["message"]
            contexte = result["contexte"]

            # Format historique Gradio
            history = history or []
            history.append([message, f"{response}\n\n*[Contexte: {contexte}]*"])

            return "", history

        except Exception as e:
            error_msg = f"Erreur: {str(e)}"
            history = history or []
            history.append([message, error_msg])
            return "", history

    # Interface Gradio avec onglets
    with gr.Blocks(title="3 Philosophes - Test Prompts") as interface:
        gr.Markdown("# ðŸŽ­ 3 Philosophes - 1 ModÃ¨le, 3 Prompts SystÃ¨me")
        gr.Markdown("*Qwen 14B + LoRA Spinoza NB, personnalisÃ© par prompts systÃ¨me*")

        with gr.Tabs():
            # Onglet Spinoza
            with gr.TabItem("Spinoza"):
                chatbot_spinoza = gr.Chatbot(
                    value=initialiser_conversation("spinoza"),
                    height=500
                )
                msg_spinoza = gr.Textbox(placeholder="RÃ©ponds Ã  Spinoza...")
                nouveau_spinoza = gr.Button("ðŸŽ² Nouvelle question")

                msg_spinoza.submit(
                    lambda m, h: chat_function(m, h, "spinoza"),
                    [msg_spinoza, chatbot_spinoza],
                    [msg_spinoza, chatbot_spinoza]
                )
                nouveau_spinoza.click(
                    lambda: initialiser_conversation("spinoza"),
                    None,
                    chatbot_spinoza
                )

            # Onglet Bergson
            with gr.TabItem("Bergson"):
                chatbot_bergson = gr.Chatbot(
                    value=initialiser_conversation("bergson"),
                    height=500
                )
                msg_bergson = gr.Textbox(placeholder="RÃ©ponds Ã  Bergson...")
                nouveau_bergson = gr.Button("ðŸŽ² Nouvelle question")

                msg_bergson.submit(
                    lambda m, h: chat_function(m, h, "bergson"),
                    [msg_bergson, chatbot_bergson],
                    [msg_bergson, chatbot_bergson]
                )
                nouveau_bergson.click(
                    lambda: initialiser_conversation("bergson"),
                    None,
                    chatbot_bergson
                )

            # Onglet Kant
            with gr.TabItem("Kant"):
                chatbot_kant = gr.Chatbot(
                    value=initialiser_conversation("kant"),
                    height=500
                )
                msg_kant = gr.Textbox(placeholder="RÃ©ponds Ã  Kant...")
                nouveau_kant = gr.Button("ðŸŽ² Nouvelle question")

                msg_kant.submit(
                    lambda m, h: chat_function(m, h, "kant"),
                    [msg_kant, chatbot_kant],
                    [msg_kant, chatbot_kant]
                )
                nouveau_kant.click(
                    lambda: initialiser_conversation("kant"),
                    None,
                    chatbot_kant
                )

        gr.Markdown("---")
        gr.Markdown("**API REST:** GET `/health`, POST `/chat`, GET `/init/{philosopher}`")
        gr.Markdown("**Philosophers:** spinoza, bergson, kant")

    return interface

# ============================================
# LANCEMENT
# ============================================

if __name__ == "__main__":
    print("ðŸ”„ Initialisation modÃ¨le...")
    model, tokenizer = load_model()

    print("ðŸ”„ CrÃ©ation interface Gradio...")
    interface = create_interface(model, tokenizer)

    # Monter l'API FastAPI sur Gradio
    app = gr.mount_gradio_app(api, interface, path="/")

    print("âœ… Lancement serveur (Gradio + FastAPI)...")
    print("ðŸ“¡ Interface Gradio: http://0.0.0.0:7860")
    print("ðŸ“¡ API REST: http://0.0.0.0:7860/chat")
    print("ðŸŽ­ 3 philosophes: Spinoza, Bergson, Kant")

    uvicorn.run(app, host="0.0.0.0", port=7860)
