# üéØ Calibration Ma√Øeuthon - Guide d'Utilisation

**Script :** `calibrate_evaluator.py`  
**Documentation :** `../docs/references/calibration-evaluation.md`

---

## üìã √âtat Actuel

### ‚úÖ Ce qui est en place

1. **Script de calibration** : `ML/calibrate_evaluator.py`
   - 5 avatars pr√©d√©finis (bons, mauvais, moyens)
   - Dialogue r√©el int√©gr√© (avatar_2_medium)
   - Fonctions de comparaison et m√©triques

2. **Dialogues de r√©f√©rence** : `ML/dialogue-reel-1.txt`
   - Dialogue r√©el extrait des logs
   - Score frontend : 127
   - Utilis√© pour cr√©er l'avatar_2_medium

3. **Routine d'√©valuation** : Backend `/evaluate`
   - Endpoint disponible dans `Backend/RAG_Spinoza_secours.ipynb`
   - √âvalue sur 3 crit√®res : Compr√©hension, Coop√©ration, Progression

---

## üöÄ Lancer la Calibration

### √âtape 1 : V√©rifier que le Backend tourne

1. **Ouvrir** le notebook Colab `Backend/RAG_Spinoza_secours.ipynb`
2. **Ex√©cuter** toutes les cellules jusqu'√† l'endpoint `/evaluate`
3. **R√©cup√©rer** l'URL ngrok g√©n√©r√©e (ex: `https://xxx.ngrok-free.dev`)

### √âtape 2 : Mettre √† jour l'URL dans le script

Modifier la ligne 17 dans `calibrate_evaluator.py` :

```python
API_BASE_URL = "https://xxx.ngrok-free.dev"  # Remettre votre URL ngrok
ENDPOINT_EVALUATE = f"{API_BASE_URL}/evaluate"
```

### √âtape 3 : Lancer le script

```bash
cd Spinoza_Secours_HF/ML
python calibrate_evaluator.py https://xxx.ngrok-free.dev/evaluate
```

**Ou sans argument** (utilise l'URL par d√©faut) :

```bash
python calibrate_evaluator.py
```

---

## üìä Interpr√©ter les R√©sultats

### M√©triques affich√©es

Le script affiche :
- **Erreur moyenne** par crit√®re (compr√©hension, coop√©ration, progression)
- **Erreur max** par crit√®re
- **Erreur totale** moyenne

### Seuil acceptable

- ‚úÖ **Erreur < 2 points** : Calibration acceptable
- ‚ö†Ô∏è **Erreur 2-3 points** : Calibration √† am√©liorer
- ‚ùå **Erreur > 3 points** : Ajustement n√©cessaire

### Recommandations automatiques

Le script propose des recommandations si l'erreur moyenne > 2 :
- ‚ö†Ô∏è Compr√©hension sous/sur-√©valu√©e ‚Üí Ajuster le prompt pour clarifier ce crit√®re
- ‚ö†Ô∏è Coop√©ration sous/sur-√©valu√©e ‚Üí Ajouter des exemples de coop√©ration dans le prompt
- ‚ö†Ô∏è Progression sous/sur-√©valu√©e ‚Üí Clarifier la d√©finition de la progression dans le prompt

---

## üìù R√©sultats Sauvegard√©s

Le script sauvegarde automatiquement les r√©sultats dans :
- **`calibration_results.json`** (dans le dossier ML/)

**Contenu :**
```json
{
  "metrics": {
    "mean_error_comprehension": 1.5,
    "mean_error_cooperation": 2.1,
    "mean_error_progression": 1.8,
    ...
  },
  "results": [
    {
      "avatar": {...},
      "generated": {...},
      "expected": {...},
      "errors": {...}
    },
    ...
  ]
}
```

---

## üîß Ajuster la Calibration

Si l'erreur est trop √©lev√©e, ajuster :

### 1. Le Prompt d'√âvaluation

Modifier `PROMPT_EVALUATION` dans `Backend/RAG_Spinoza_secours.ipynb` :

```python
PROMPT_EVALUATION = """Tu es Spinoza. Voici l'√©change complet avec un √©l√®ve :

{dialogue}

√âvalue l'√©l√®ve sur 3 crit√®res (0 √† 10) :
1. Compr√©hension de tes id√©es (signes : reformule, pose des questions pertinentes, montre qu'il comprend)
2. Coop√©ration dans le dialogue (signes : √©coute, r√©pond aux questions, ne r√©siste pas syst√©matiquement)
3. Progression de la pens√©e (signes : fait des liens, avance dans sa r√©flexion, progresse)

[...]
"""
```

### 2. La Temp√©rature

Dans `evaluer_dialogue()`, ajuster la temp√©rature :
- **Temp√©rature basse (0.1-0.2)** : Plus strict, moins de variation
- **Temp√©rature haute (0.3-0.5)** : Plus flexible, plus de variation

### 3. Les Poids des Crit√®res

Si un crit√®re est syst√©matiquement sous/sur-√©valu√©, ajouter des exemples dans le prompt pour ce crit√®re.

---

## üìã Avatars Actuels

### Avatar 1 : Bon √©l√®ve
- **Type :** good
- **Scores attendus :** Compr√©hension 9, Coop√©ration 9, Progression 9, Total 27

### Avatar 2 : Dialogue r√©el (moyen)
- **Type :** medium
- **Scores attendus :** Compr√©hension 6, Coop√©ration 7, Progression 7, Total 20
- **Dialogue :** Dialogue r√©el extrait des logs (dialogue-reel-1.txt)

### Avatar 3 : Mauvais √©l√®ve
- **Type :** bad
- **Scores attendus :** Compr√©hension 1, Coop√©ration 1, Progression 0, Total 2

### Avatar 4 : Excellent √©l√®ve progressif
- **Type :** good
- **Scores attendus :** Compr√©hension 10, Coop√©ration 10, Progression 10, Total 30

### Avatar 5 : √âl√®ve r√©sistant
- **Type :** bad
- **Scores attendus :** Compr√©hension 4, Coop√©ration 5, Progression 3, Total 12

---

## üéØ Prochaines √âtapes

1. **Lancer la calibration** avec le script actuel
2. **Analyser les r√©sultats** et identifier les crit√®res √† ajuster
3. **Modifier les prompts** dans le notebook Colab si n√©cessaire
4. **Relancer la calibration** pour v√©rifier les am√©liorations
5. **It√©rer** jusqu'√† obtenir une erreur acceptable (< 2 points)

---

## üí° Astuces

- **Tester avec un seul avatar d'abord** : Modifier le script pour ne tester qu'un avatar (ex: avatar_2_medium) pour valider rapidement
- **Ajouter des avatars** : Si besoin, ajouter plus d'avatars dans la liste `AVATARS` du script
- **Comparer avec les logs** : Utiliser les dialogues r√©els des logs pour cr√©er de nouveaux avatars

---

**Pr√™t √† lancer !** Il suffit de mettre √† jour l'URL ngrok et de lancer le script.

