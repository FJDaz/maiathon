# üéÆ Cellule Colab : √âvaluation Incr√©mentale (Fil de l'Eau)

**√Ä ajouter APR√àS la cellule `/evaluate` (Ma√Øeuthon) et AVANT la cellule de Lancement Serveur**

---

## üìù Code de la Cellule

**‚ö†Ô∏è Fichier de r√©f√©rence :** `Backend/CELLULE_EVALUATION_INCREMENTALE.py` (code brut pr√™t √† copier)

**Code √† copier dans Colab :**

```python
# =============================================================================
# ‚ö° √âVALUATION INCR√âMENTALE - Fil de l'Eau (Optimisation Inf√©rence)
# =============================================================================

# Prompt √©valuation incr√©mentale (court, rapide)
PROMPT_EVALUATION_INCREMENTAL = """√âvalue rapidement (0-10) :
- Compr√©hension : Comprend-il mes id√©es ?
- Coop√©ration : Coop√®re-t-il dans le dialogue ?
- Progression : Sa pens√©e progresse-t-elle ?

Dialogue r√©cent (2 derniers √©changes) :
{dialogue_recent}

JSON strict (aucune prose) :
{{
 "comprehension": X,
 "cooperation": Y,
 "progression": Z,
 "total": X+Y+Z
}}"""

# Stockage scores incr√©mentaux (en m√©moire)
# Structure : {dialogue_id: [scores_√©change_2, scores_√©change_4, ...]}
incremental_scores = {}

def evaluer_incremental(dialogue: str) -> dict:
    """
    √âvaluation l√©g√®re au fil de l'eau (tous les 2 √©changes)
    - Prompt court (2 derniers √©changes seulement)
    - Temp√©rature basse (0.1) - Strict pour JSON
    - Max tokens r√©duit (50) - Rapidit√©
    - Pas de message final - Gain de temps
    """
    # Extraire les 2 derniers √©changes seulement (4 lignes : √âl√®ve + Spinoza x2)
    lines = [l.strip() for l in dialogue.split('\n') if l.strip()]
    if len(lines) > 4:
        recent_exchanges = '\n'.join(lines[-4:])  # 2 derniers √©changes
    else:
        recent_exchanges = dialogue
    
    prompt_eval = PROMPT_EVALUATION_INCREMENTAL.format(dialogue_recent=recent_exchanges)
    
    # Formatage Mistral
    prompt_eval_formatted = f"<s>[INST] {prompt_eval} [/INST]"
    
    inputs = tokenizer(prompt_eval_formatted, return_tensors="pt").to(model.device)
    input_length = inputs['input_ids'].shape[1]
    
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    # Inf√©rence rapide (temp√©rature basse, tokens r√©duits)
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,  # Court pour rapidit√©
            temperature=0.1,    # Strict pour JSON
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens = outputs[0][input_length:]
    reponse_eval = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    # Parser JSON (am√©lior√© pour capturer m√™me avec texte autour)
    json_pattern = r'\{[^{}]*"comprehension"[^{}]*"cooperation"[^{}]*"progression"[^{}]*"total"[^{}]*\}'
    json_match = re.search(json_pattern, reponse_eval, re.DOTALL)
    
    if json_match:
        try:
            details_model = json.loads(json_match.group(0))
            # Valider que tous les champs sont pr√©sents
            required_fields = ["comprehension", "cooperation", "progression", "total"]
            for field in required_fields:
                if field not in details_model:
                    details_model[field] = 5 if field != "total" else 15
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Erreur parsing JSON incr√©mental: {e}")
            print(f"   R√©ponse brute: {reponse_eval[:200]}")
            details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
    else:
        print(f"‚ö†Ô∏è JSON non trouv√© dans r√©ponse incr√©mentale: {reponse_eval[:200]}")
        details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
    
    return details_model

# Endpoint FastAPI pour √©valuation incr√©mentale
@app.post("/evaluate/incremental")
def evaluate_incremental(req: EvaluateRequest):
    """
    √âvaluation l√©g√®re au fil de l'eau (tous les 2 √©changes)
    - Appel√© par le frontend apr√®s chaque 2 √©changes
    - Stocke les scores pour l'√©valuation finale
    - Retourne seulement les scores (pas de message final)
    """
    # √âvaluer le dialogue r√©cent
    details_model = evaluer_incremental(req.dialogue)
    
    # Stocker pour l'√©valuation finale
    # Utiliser un ID simple bas√© sur le hash du dialogue
    # En production, utiliser un vrai ID de session
    dialogue_id = hash(req.dialogue)
    
    if dialogue_id not in incremental_scores:
        incremental_scores[dialogue_id] = []
    
    incremental_scores[dialogue_id].append({
        "scores": details_model,
        "exchange_count": len(incremental_scores[dialogue_id]) + 1
    })
    
    return {
        "scores": details_model,
        "exchange_count": len(incremental_scores[dialogue_id]),
        "accumulated": len(incremental_scores[dialogue_id]) > 0
    }

print("‚úÖ Endpoint /evaluate/incremental cr√©√© pour √©valuation au fil de l'eau")
```

---

## üîß Modification de l'√âvaluation Finale

**Modifier la fonction `evaluer_dialogue()` existante** pour utiliser les scores incr√©mentaux :

```python
def evaluer_dialogue(dialogue: str, score_front: int) -> dict:
    """
    √âvalue le dialogue et g√©n√®re le message final.
    Utilise les scores incr√©mentaux si disponibles pour optimiser.
    """
    dialogue_id = hash(dialogue)
    
    # Si scores incr√©mentaux disponibles, les utiliser comme base
    if dialogue_id in incremental_scores and len(incremental_scores[dialogue_id]) > 0:
        scores_inc = incremental_scores[dialogue_id]
        
        # Agr√©ger les scores incr√©mentaux (moyenne pond√©r√©e par ordre)
        # Le dernier √©change est plus important (poids croissant)
        n = len(scores_inc)
        weights = [i+1 for i in range(n)]  # [1, 2, 3, ...]
        total_weight = sum(weights)
        
        # Calculer moyenne pond√©r√©e
        details_model = {
            "comprehension": int(round(
                sum(s["scores"]["comprehension"] * w for s, w in zip(scores_inc, weights)) / total_weight
            )),
            "cooperation": int(round(
                sum(s["scores"]["cooperation"] * w for s, w in zip(scores_inc, weights)) / total_weight
            )),
            "progression": int(round(
                sum(s["scores"]["progression"] * w for s, w in zip(scores_inc, weights)) / total_weight
            ))
        }
        details_model["total"] = details_model["comprehension"] + details_model["cooperation"] + details_model["progression"]
        
        print(f"üìä Scores incr√©mentaux utilis√©s: {len(scores_inc)} √©valuations")
        
        # Score total
        score_backend = details_model.get("total", 15)
        score_final = score_front + score_backend
        
        # Message final uniquement (scores d√©j√† calcul√©s)
        # ... (code g√©n√©ration message final existant) ...
        
        return {
            "score_final": score_final,
            "message_final": message_final,
            "details_model": details_model,
            "used_incremental": True  # Flag pour debug
        }
    
    # Sinon, √©valuation classique (fallback si pas de scores incr√©mentaux)
    # ... (code existant d'√©valuation compl√®te) ...
```

---

## üìã Instructions d'Ajout dans Colab

### √âtape 1 : Ajouter la Cellule d'√âvaluation Incr√©mentale

1. **Ouvrir** le notebook `RAG_Spinoza_secours.ipynb` dans Colab
2. **Trouver** la cellule avec `/evaluate` (apr√®s la cellule 7)
3. **Ins√©rer une nouvelle cellule** juste apr√®s cette cellule
4. **Copier-coller** le code de l'√©valuation incr√©mentale
5. **Ex√©cuter** la cellule

### √âtape 2 : Modifier l'√âvaluation Finale

1. **Modifier** la fonction `evaluer_dialogue()` existante
2. **Ajouter** la logique pour utiliser les scores incr√©mentaux
3. **Tester** avec un dialogue complet

---

## ‚úÖ V√©rification

Apr√®s ajout, tester avec :

```bash
# Test √©valuation incr√©mentale
curl -X POST https://ton-url-ngrok.ngrok-free.dev/evaluate/incremental \
  -H "Content-Type: application/json" \
  -d '{"dialogue": "√âl√®ve: Bonjour\nSpinoza: Salut\n√âl√®ve: Qu'est-ce que la libert√© ?", "score_front": 100}'
```

**R√©sultat attendu :**
```json
{
  "scores": {
    "comprehension": 7,
    "cooperation": 8,
    "progression": 6,
    "total": 21
  },
  "exchange_count": 1,
  "accumulated": true
}
```

---

## üéØ B√©n√©fices

1. ‚úÖ **Charge distribu√©e** : Pas de pic en fin de dialogue
2. ‚úÖ **Moins de fatigue** : √âvaluation de segments courts
3. ‚úÖ **D√©tection pr√©coce** : Probl√®mes identifi√©s t√¥t
4. ‚úÖ **√âvaluation finale optimis√©e** : Utilise les scores pr√©-calcul√©s

---

**Note :** L'√©valuation incr√©mentale est **invisible** √† l'utilisateur (pas de feedback visuel pendant le dialogue) pour pr√©server la qualit√© du dialogue.

---

**Document cr√©√© le :** 21 novembre 2025

