# üìù Proposition : Code Colab pour Spinoza Secours

**Date :** 21 novembre 2025  
**Status :** ‚è∏Ô∏è **PROPOSITION** - En attente validation  
**‚ö†Ô∏è NE PAS MODIFIER LE CODE COLAB SANS VALIDATION**

---

## üéØ Objectif

Cr√©er un code Colab complet qui utilise le **prompt syst√®me hybride optimis√©** pour Spinoza Secours.

---

## üìã Contenu Propos√©

### 1. **Installation D√©pendances**
```python
!pip install -q pyngrok fastapi uvicorn transformers peft accelerate bitsandbytes torch
```

### 2. **Imports**
```python
from pyngrok import ngrok
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import threading, uvicorn, random, time, re, torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
```

### 3. **Configuration ngrok**
```python
NGROK_TOKEN = "TON_TOKEN_ICI"
ngrok.set_auth_token(NGROK_TOKEN)
!lsof -ti:8000 | xargs kill -9 2>/dev/null; sleep 1
```

### 4. **Prompt Syst√®me Hybride** (depuis `prompt_systeme_hybride.py`)
- `SYSTEM_PROMPT_SPINOZA` (~250 tokens)
- `INSTRUCTIONS_CONTEXTUELLES` (accord/confusion/r√©sistance/neutre)
- `INSTRUCTION_RAG` (optionnel)
- `construire_prompt_complet(contexte, use_rag_instruction=True)`

### 5. **D√©tection Contexte**
```python
def detecter_contexte(user_input: str) -> str:
    # Retourne "accord", "confusion", "resistance", "neutre"
```

### 6. **Post-Processing**
- `nettoyer_reponse(text)` - Nettoie annotations, emojis, espaces
- `limiter_phrases(text, max_phrases=3)` - Limite √† 3 phrases

### 7. **Chargement Mod√®le**
```python
@torch.no_grad()
def load_model():
    # Charge Mistral 7B + LoRA
    # Device: CUDA (T4 sur Colab) avec quantization 4-bit
    # Adapter: "FJDaz/mistral-7b-philosophes-lora"
    return model, tokenizer
```

### 8. **Fonction `spinoza_repond(message)`**
- D√©tecte contexte
- Construit prompt adaptatif
- G√©n√®re r√©ponse avec mod√®le
- Post-processe
- Retourne r√©ponse nettoy√©e

### 9. **API FastAPI**
- `/health` - V√©rification √©tat
- `/init` - Initialisation conversation
- `/chat` - POST avec message utilisateur
- CORS configur√©

### 10. **Lancement Serveur + ngrok**
- Thread background pour FastAPI
- Tunnel ngrok sur port 8000
- Affichage URL publique

---

## ‚öôÔ∏è Param√®tres Configurables

| Param√®tre | Valeur Propos√©e | Alternative |
|-----------|----------------|-------------|
| `max_new_tokens` | 150 | 100-200 selon besoin |
| `temperature` | 0.7 | 0.5-0.9 |
| `top_p` | 0.9 | 0.8-0.95 |
| `use_rag_instruction` | `True` | ‚úÖ Instructions seulement (pas d'injection) |
| `device` | `"cuda"` | T4 sur Colab |
| `adapter_name` | `"FJDaz/mistral-7b-philosophes-lora"` | ‚úÖ Confirm√© |

---

## üìä Estimation Tokens

| Composant | Tokens |
|-----------|--------|
| Prompt syst√®me base | ~250 |
| Instruction contextuelle | ~30-50 |
| Instruction RAG | ~50 |
| **Total prompt** | **~330-350** |
| Historique (4 √©changes) | ~300 |
| Message utilisateur | ~50 |
| **Total par requ√™te** | **~680-700** |

---

## ‚úÖ Avantages

1. **Prompt optimis√©** : ~250 tokens (vs ~400 pour version compl√®te)
2. **Adaptatif** : S'adapte au contexte (accord/confusion/r√©sistance/neutre)
3. **Premi√®re personne** : Explicite dans le prompt
4. **Sch√®mes logiques** : Int√©gr√©s dans le prompt
5. **√âconomie tokens** : RAG par instructions (pas d'injection)

---

## ‚ö†Ô∏è Points d'Attention

1. **Token ngrok** : √Ä remplacer par le vrai token
2. **Adapter LoRA** : V√©rifier le nom exact de l'adapter
3. **Device** : CPU par d√©faut (changer en CUDA si GPU)
4. **RAG** : Actuellement par instructions (pas d'injection passages)

---

## üîÑ Modifications Possibles

### Option A : RAG Disabled (√âconomie Max)
```python
# Dans spinoza_repond()
system_prompt = construire_prompt_complet(contexte, use_rag_instruction=False)
```
**√âconomie :** ~50 tokens

### Option B : Prompt Minimal (√âconomie Max)
```python
# Utiliser version minimaliste (~80 tokens)
SYSTEM_PROMPT_MINIMAL = """Tu ES Spinoza. Premi√®re personne. Tutoie l'√©l√®ve..."""
```
**√âconomie :** ~170 tokens

### Option C : RAG S√©lectif (Si besoin)
```python
# Ajouter recherche RAG si contexte confusion/accord
if contexte in ["confusion", "accord"]:
    # Recherche RAG + injection passages
```

---

## üìù Structure Fichier Propos√©e

```
colab_spinoza_secours_complet.py
‚îú‚îÄ‚îÄ Installation d√©pendances
‚îú‚îÄ‚îÄ Imports
‚îú‚îÄ‚îÄ Config ngrok
‚îú‚îÄ‚îÄ Prompt syst√®me hybride
‚îú‚îÄ‚îÄ D√©tection contexte
‚îú‚îÄ‚îÄ Post-processing
‚îú‚îÄ‚îÄ Chargement mod√®le
‚îú‚îÄ‚îÄ Fonction spinoza_repond()
‚îú‚îÄ‚îÄ API FastAPI
‚îî‚îÄ‚îÄ Lancement serveur + ngrok
```

---

## üîß Script Suppl√©mentaire Propos√©

### Objectif
Script s√©par√© pour tester/valider le prompt syst√®me **SANS toucher** au chargement du mod√®le ni √† l'API.

### Contenu Propos√©

#### Option 1 : Script de Test Prompt (‚úÖ CHOISI)

```python
# test_prompt_systeme.py
# - Teste le prompt syst√®me avec diff√©rents contextes
# - Affiche le prompt g√©n√©r√© (sans g√©n√©ration mod√®le)
# - Valide la structure du prompt
# - Estime les tokens
```

**Fonctions :**
- `test_prompt_contextes()` - Teste tous les contextes (accord/confusion/r√©sistance/neutre)
- `afficher_prompt(contexte)` - Affiche le prompt g√©n√©r√©
- `estimer_tokens(prompt)` - Estime le nombre de tokens
- `valider_structure(prompt)` - V√©rifie que le prompt contient les √©l√©ments requis

**Avantages :**
- ‚úÖ Teste le prompt sans charger le mod√®le
- ‚úÖ Rapide (pas d'inference)
- ‚úÖ Permet de valider avant utilisation r√©elle
- ‚úÖ **Ind√©pendant du frontend** (teste juste le prompt)

**Note BM25 :** Le test BM25 (Lunr.js) n√©cessite le frontend. Workflow :
1. **Colab (Option 1)** ‚Üí Test prompt syst√®me ‚Üí Validation prompt
2. **Frontend + API** ‚Üí Test BM25 (Lunr.js) ‚Üí Validation RAG c√¥t√© client

#### Option 2 : Script Utilitaires (Si besoin)
```python
# utils_prompt.py
# - Fonctions utilitaires pour le prompt
# - Formatage, validation, etc.
```

**Fonctions possibles :**
- `formater_prompt(prompt, contexte)` - Formatage avanc√©
- `valider_premiere_personne(prompt)` - V√©rifie premi√®re personne
- `extraire_schemes(prompt)` - Extrait les sch√®mes logiques mentionn√©s

**‚ö†Ô∏è Suggestion :** Seulement si vraiment n√©cessaire

---

## ‚úÖ R√©ponses Validation

1. **Adapter LoRA** : `"FJDaz/mistral-7b-philosophes-lora"` (trouv√© dans `app.py`)
2. **Device** : **CUDA** (T4 sur Colab) - Utiliser `device="cuda"` avec quantization 4-bit
3. **RAG** : **Instructions seulement** (pas d'injection passages)
4. **Tokens** : **Priorit√© qualit√©** (pas d'√©conomie maximale)
5. **Param√®tres g√©n√©ration** : `max_new_tokens=150` pour d√©marrer, ajustable selon tests
6. **Script suppl√©mentaire** : **Option 1 (Test Prompt)** - Tests en Colab avant frontend

### üìù Clarification BM25 (Lunr.js)

**Question :** Besoin de frontend pour tester BM25 en m√™me temps ?

**R√©ponse :**
- **Script Option 1 (Test Prompt)** : Teste le prompt syst√®me **sans mod√®le ni frontend** (rapide, validation structure)
- **Test BM25 (Lunr.js)** : N√©cessite le **frontend** (`index_spinoza.html`) car c'est c√¥t√© client (JavaScript)
- **Recommandation** : 
  1. D'abord tester le prompt en Colab (Option 1) - validation prompt
  2. Ensuite tester BM25 avec frontend - validation RAG c√¥t√© client

**Workflow propos√© :**
```
Colab (Option 1) ‚Üí Test prompt syst√®me ‚Üí ‚úÖ Prompt valid√©
    ‚Üì
Frontend + API ‚Üí Test BM25 (Lunr.js) ‚Üí ‚úÖ RAG valid√©
```

---

**Status :** ‚è∏Ô∏è En attente validation avant impl√©mentation

