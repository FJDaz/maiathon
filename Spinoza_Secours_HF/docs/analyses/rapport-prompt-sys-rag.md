# üìä Rapport d'Analyse : Prompt Syst√®me & RAG pour Spinoza Secours

**Date :** 21 novembre 2025  
**Contexte :** Spinoza Secours HF (Colab + ngrok + Mistral 7B FT)  
**Contrainte critique :** Derniers jetons disponibles - √©conomie n√©cessaire

---

## üîç √âtat Actuel des Prompts

### 1. Prompts Impl√©ment√©s (R√©f√©rences)

#### A. Version `bergsonAndFriends_HF/app_with_api.py` (V2)
```python
SYSTEM_PROMPTS_BASE = [
    """Tu es Spinoza incarn√©. Tu dialogues avec un √©l√®ve pour le guider vers la compr√©hension.
Utilise les sch√®mes logiques pour structurer ton raisonnement.
Varie tes transitions: "Donc", "mais alors", "Imagine", "Cela implique", etc.
Sois p√©dagogique mais rigoureux. Pose des questions pour faire r√©fl√©chir.""",
    # + 2 autres variantes courtes
]
```
**Caract√©ristiques :**
- ‚úÖ Court (~150 tokens)
- ‚úÖ D√©tection contexte (accord/confusion/r√©sistance/neutre)
- ‚úÖ Adaptatif selon contexte
- ‚ö†Ô∏è Pas de sch√®mes logiques d√©taill√©s
- ‚ö†Ô∏è Pas de premi√®re personne explicite

#### B. Version `3_PHI_HF/app.py` (Compl√®te)
```python
SYSTEM_PROMPTS = {
    "spinoza": """Tu ES Spinoza incarn√©. Tu dialogues avec un √©l√®ve de Terminale en premi√®re personne.

TON STYLE :
- G√©om√©trie des affects : tu r√©v√®les les causes n√©cessaires, tu d√©duis
- Tu enseignes que Dieu = Nature
- Ton vocabulaire : conatus, affects, puissance d'agir, b√©atitude, servitude

TES SCH√àMES LOGIQUES :
- Identit√© : Dieu = Nature = Substance unique
- Identit√© : Libert√© = Connaissance de la n√©cessit√©
- Implication : Si joie ‚Üí augmentation puissance
- Causalit√© : Tout a une cause n√©cessaire (pas de libre arbitre)

TA M√âTHODE :
1. Tu r√©v√®les la n√©cessit√© causale
2. Tu distingues servitude (ignorance) vs libert√© (connaissance)
3. Tu utilises des exemples concrets modernes (r√©seaux sociaux, affects quotidiens)

TRANSITIONS √Ä VARIER :
- "Donc" (pour d√©ductions logiques)
- "mais alors" (pour r√©v√©ler contradictions - UTILISER SOUVENT)
- "Imagine" (pour analogies concr√®tes)
- "Cela implique" (pour implications n√©cessaires)
- "Attends. Tu dis X mais tu fais Y. Comment tu expliques ?"
- "T'as raison sur [point]. mais alors [tension]..."
- "Pourtant", "Cependant", "Or", "Sauf que"
- "Attends, c'est contradictoire :", "Il y a une tension ici :"

FORMULES DIALECTIQUES SPINOZISTES :
- "mais alors, as-tu conscience des CAUSES de tes choix ?"
- "Si tu ignores les causes, alors tu crois √™tre libre (mais tu te trompes)"
- "Ignorance causes ‚Üí Illusion libert√©"
- "Si libre arbitre, alors effet sans cause. Mais la Nature ne conna√Æt pas d'effet sans cause."

FORMULES P√âDAGOGIQUES :
- "Je comprends. Mais regarde..."
- "OK. Alors toi, comment tu vois √ßa ?"
- "C'est vrai, mais est-ce que c'est tout ?"

Tu r√©ponds de mani√®re conversationnelle, tu tutoies l'√©l√®ve, tu d√©montres g√©om√©triquement.
Ne parle JAMAIS de toi √† la 3√®me personne. Tu ES Spinoza."""
}
```
**Caract√©ristiques :**
- ‚úÖ Complet (~400 tokens)
- ‚úÖ Premi√®re personne explicite
- ‚úÖ Sch√®mes logiques d√©taill√©s
- ‚úÖ Formules dialectiques nombreuses
- ‚úÖ Style conversationnel

### 2. Documents de R√©f√©rence

#### `POLITIQUE_PROMPTS_SCHEMES_LOGiques.md`
- ‚úÖ Guide complet d'impl√©mentation
- ‚úÖ Sch√®mes logiques par philosophe
- ‚úÖ D√©tection contexte
- ‚úÖ Construction prompts adaptatifs

#### `ENRICHSISSEMENT_PROMPT_SYS_SNB.md`
- ‚úÖ Formules dialectiques ("mais alors", etc.)
- ‚úÖ Climax dialectique avec conditions
- ‚ö†Ô∏è Format JSON (pas directement utilisable)

---

## üéØ Capacit√©s du Mod√®le (Mistral 7B FT)

### Fen√™tre Contextuelle

**Mistral 7B :**
- **Context window :** 32,000 tokens (8K tokens pratique recommand√©)
- **Mod√®le fine-tun√© :** Mistral 7B + LoRA Schemes (Niveau A)

**Utilisation actuelle :**
- Prompt syst√®me : ~400 tokens (version compl√®te)
- Historique : 4 derniers √©changes (~200-400 tokens)
- Message utilisateur : ~50 tokens
- **Total par requ√™te :** ~650-850 tokens

**Marge disponible :**
- Pour RAG : ~2,000-3,000 tokens suppl√©mentaires possibles
- Pour prompt enrichi : ~1,000-2,000 tokens suppl√©mentaires possibles

### Limitations

1. **Jetons limit√©s** : √âconomie critique n√©cessaire
2. **Mod√®le 7B** : Moins puissant que Qwen 14B, mais plus rapide
3. **LoRA Schemes** : Appris sur sch√®mes logiques, pas sur style conversationnel
4. **Fine-tuning r√©cent** : Peut n√©cessiter ajustements

---

## üí° Recommandations Prompt Syst√®me

### Option 1 : Prompt Hybride Optimis√© (RECOMMAND√â)

**Strat√©gie :** Combiner le meilleur des deux versions, optimis√© pour √©conomie de tokens.

```python
SYSTEM_PROMPT_SPINOZA = """Tu ES Spinoza incarn√©. Tu dialogues avec un √©l√®ve de Terminale en premi√®re personne.

STYLE SPINOZIEN :
- G√©om√©trie des affects : r√©v√®le causes n√©cessaires, d√©duis
- Dieu = Nature
- Vocabulaire : conatus, affects, puissance d'agir, servitude

SCH√àMES LOGIQUES :
- Identit√© : Libert√© = Connaissance n√©cessit√©
- Causalit√© : Tout a cause n√©cessaire
- Implication : Joie ‚Üí augmentation puissance

M√âTHODE :
1. R√©v√®le n√©cessit√© causale
2. Distingue servitude (ignorance) vs libert√© (connaissance)
3. Exemples concrets modernes

TRANSITIONS (VARIE) :
- "Donc", "mais alors", "Imagine", "Cela implique"
- "Pourtant", "Sauf que", "C'est contradictoire"

R√àGLES :
- Tutoie (tu/ton/ta)
- Concis (2-3 phrases MAX)
- Questionne au lieu d'affirmer
- Ne parle JAMAIS de toi √† la 3√®me personne. Tu ES Spinoza."""
```

**Avantages :**
- ‚úÖ ~250 tokens (√©conomie vs version compl√®te)
- ‚úÖ Premi√®re personne explicite
- ‚úÖ Sch√®mes logiques essentiels
- ‚úÖ Transitions vari√©es
- ‚úÖ Style conversationnel

**Tokens estim√©s :** ~250 tokens (vs ~400 pour version compl√®te)

### Option 2 : Prompt Minimaliste (√âCONOMIE MAX)

```python
SYSTEM_PROMPT_MINIMAL = """Tu ES Spinoza. Premi√®re personne. Tutoie l'√©l√®ve.

Sch√®mes : Libert√© = Connaissance n√©cessit√©. Tout a cause n√©cessaire.
Transitions : "Donc", "mais alors", "Imagine" (varie).
Concis (2-3 phrases). Questionne. Ne parle JAMAIS de toi √† la 3√®me personne."""
```

**Tokens estim√©s :** ~80 tokens  
**Risque :** Perte de qualit√©/style

### Option 3 : Prompt Enrichi Progressif

**Strat√©gie :** Prompt de base + enrichissement contextuel dynamique

```python
BASE_PROMPT = """Tu ES Spinoza. Premi√®re personne. Tutoie. Concis. Questionne."""

ENRICHISSEMENTS = {
    "confusion": "Donne analogie concr√®te simple.",
    "resistance": "R√©v√®le contradiction avec 'mais alors'.",
    "accord": "Valide puis AVANCE avec 'Donc'.",
    "neutre": "Pose question pour faire r√©fl√©chir."
}

SCH√àMES_CONTEXTUELS = {
    "resistance": "Sch√®me causalit√© : Si libre arbitre ‚Üí effet sans cause. Mais Nature ne conna√Æt pas effet sans cause.",
    "confusion": "Sch√®me identit√© : Libert√© = Connaissance n√©cessit√©. Si ignores causes ‚Üí illusion libert√©.",
    # etc.
}
```

**Avantages :**
- Prompt de base l√©ger (~50 tokens)
- Enrichissement selon contexte (~50-100 tokens)
- Total : ~100-150 tokens par requ√™te

---

## üîç Recommandations RAG

### Contraintes Identifi√©es

1. **Style cass√©** : Passages RAG bruts cassent le style conversationnel
2. **Tokens limit√©s** : Pas de surcharge
3. **Mod√®le 7B** : Moins de capacit√© que Qwen 14B

### üí° Piste Whoosh/Lunr.js C√¥t√© Client

**Analyse d√©taill√©e :** Voir `ANALYSE_WHOOSH_RAG_CLIENT.md`

**Concept :** Utiliser Lunr.js (√©quivalent JavaScript de Whoosh) c√¥t√© client pour trier/filtrer passages RAG **avant** envoi au mod√®le.

**Avantages :**
- ‚úÖ √âconomie tokens (40-60%) : envoi seulement top 1-2 passages
- ‚úÖ Rapidit√© : recherche instantan√©e (pas de latence r√©seau)
- ‚úÖ Scalabilit√© : charge serveur r√©duite

**Recommandation :** **RAG Hybride (Client + Serveur)**
- Corpus l√©ger c√¥t√© client (50-100 passages cl√©s) avec Lunr.js
- Tri top passages avant envoi
- Fallback serveur si besoin

**√âconomie estim√©e :** ~100-200 tokens par requ√™te (si RAG activ√©)

### Strat√©gie RAG Recommand√©e

#### Option A : RAG S√©lectif Intelligent (RECOMMAND√â)

**Principe :** RAG seulement quand n√©cessaire, avec extraction d'id√©es (pas texte brut).

```python
def should_use_rag(message: str, contexte: str) -> bool:
    """D√©termine si RAG n√©cessaire"""
    # Concepts complexes ‚Üí RAG utile
    concepts_complexes = ["libert√©", "causalit√©", "conatus", "affects", "servitude"]
    has_complex = any(c in message.lower() for c in concepts_complexes)
    
    # Questions courtes ‚Üí Pas besoin
    is_simple = len(message.split()) < 5
    
    # Contexte confusion/accord ‚Üí RAG utile
    needs_rag = contexte in ["confusion", "accord"]
    
    return (has_complex or needs_rag) and not is_simple

def extraire_idees_passage(passage: Dict, philosopher: str) -> str:
    """Extrait ID√âES et reformule dans style philosophe"""
    # 1. Extraire phrases principales
    # 2. Reformuler premi√®re personne, langage lyc√©en
    # 3. Retourner id√©es reformul√©es (pas texte brut)
    pass
```

**Utilisation :**
- RAG seulement si `should_use_rag()` = True
- Max 1-2 passages pertinents (score > 5)
- Extraction d'id√©es + reformulation
- Injection contextuelle selon contexte

**Tokens estim√©s :** +100-200 tokens si RAG activ√©

#### Option B : RAG Int√©gr√© dans Prompt (√âCONOMIE)

**Principe :** Instructions dans prompt syst√®me pour utiliser connaissances, sans injection de passages.

```python
SYSTEM_PROMPT_WITH_RAG_INSTRUCTION = """[... prompt base ...]

UTILISATION CONNAISSANCES :
- Tu connais l'√âthique de Spinoza
- Cite implicitement ("comme je l'ai montr√©...", "dans mon ≈ìuvre...")
- Reformule dans TON style (premi√®re personne, lyc√©en)
- Ne r√©cite pas : extrais id√©es et reformule naturellement"""
```

**Avantages :**
- ‚úÖ Pas d'injection de passages ‚Üí √©conomie tokens
- ‚úÖ Le mod√®le utilise ses connaissances acquises
- ‚úÖ Style pr√©serv√©

**Limitations :**
- ‚ö†Ô∏è D√©pend de ce que le mod√®le a appris (LoRA + connaissances de base)
- ‚ö†Ô∏è Moins pr√©cis que RAG avec passages

#### Option C : RAG Disabled (√âCONOMIE MAX)

**Principe :** Pas de RAG, uniquement prompt syst√®me + connaissances du mod√®le.

**Avantages :**
- ‚úÖ √âconomie maximale de tokens
- ‚úÖ Style conversationnel garanti
- ‚úÖ Pas de probl√®me de style cass√©

**Limitations :**
- ‚ö†Ô∏è Moins de pr√©cision sur d√©tails de l'≈ìuvre
- ‚ö†Ô∏è D√©pend uniquement des connaissances du mod√®le

---

## üìä Comparaison des Options

| Option | Tokens Prompt | Tokens RAG | Total | Qualit√© | √âconomie |
|--------|---------------|------------|-------|---------|----------|
| **Prompt Hybride + RAG S√©lectif** | ~250 | +100-200 | ~350-450 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Prompt Minimal + RAG S√©lectif** | ~80 | +100-200 | ~180-280 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Prompt Hybride + RAG Instruction** | ~300 | 0 | ~300 | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Prompt Hybride + RAG Disabled** | ~250 | 0 | ~250 | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üéØ Recommandation Finale

### Pour Spinoza Secours (Contrainte Jetons)

**Configuration recommand√©e :**

1. **Prompt Syst√®me :** Option 1 (Hybride Optimis√©) - ~250 tokens
   - Premi√®re personne explicite
   - Sch√®mes logiques essentiels
   - Transitions vari√©es
   - Style conversationnel

2. **RAG :** Option B (Instructions dans prompt) - 0 tokens suppl√©mentaires
   - Instructions pour utiliser connaissances
   - Pas d'injection de passages
   - Style pr√©serv√©
   - √âconomie maximale

3. **D√©tection contexte :** Conserv√©e (accord/confusion/r√©sistance/neutre)
   - ~50 tokens pour instructions contextuelles
   - Adapte le prompt selon contexte

**Total estim√© par requ√™te :**
- Prompt syst√®me : ~250 tokens
- Instructions contextuelles : ~50 tokens
- Historique (4 √©changes) : ~300 tokens
- Message utilisateur : ~50 tokens
- **Total : ~650 tokens par requ√™te**

**Marge disponible :** ~2,000-3,000 tokens (pour ajustements futurs)

---

## ‚ö†Ô∏è Points d'Attention

### 1. Premi√®re Personne
- ‚úÖ **Critique** : Le mod√®le doit dire "Je montre que..." pas "Pour Spinoza..."
- ‚úÖ **Solution** : Instruction explicite dans prompt + fine-tuning correction

### 2. Style Conversationnel
- ‚úÖ **Critique** : √âviter langage acad√©mique lourd
- ‚úÖ **Solution** : Instructions "langage lyc√©en, conversationnel"

### 3. Vari√©t√© des R√©ponses
- ‚úÖ **Critique** : √âviter r√©p√©tition
- ‚úÖ **Solution** : Transitions vari√©es dans prompt + temp√©rature 0.7

### 4. Adaptation Contextuelle
- ‚úÖ **Critique** : R√©pondre √† la question pos√©e
- ‚úÖ **Solution** : D√©tection contexte + instructions adaptatives

---

## üìù Plan d'Impl√©mentation

### √âtape 1 : Prompt Syst√®me Hybride
- [ ] Cr√©er `SYSTEM_PROMPT_SPINOZA` optimis√© (~250 tokens)
- [ ] Tester avec diff√©rents contextes
- [ ] V√©rifier premi√®re personne

### √âtape 2 : RAG (Optionnel)
- [ ] Si besoin : Impl√©menter RAG s√©lectif intelligent
- [ ] Sinon : Utiliser instructions dans prompt (Option B)

### √âtape 3 : Optimisation
- [ ] Ajuster selon r√©sultats
- [ ] √âconomiser tokens si n√©cessaire
- [ ] Monitorer qualit√© vs √©conomie

---

## üîó R√©f√©rences

- `POLITIQUE_PROMPTS_SCHEMES_LOGiques.md` : Guide complet
- `ENRICHSISSEMENT_PROMPT_SYS_SNB.md` : Formules dialectiques
- `3_PHI_HF/app.py` : Version compl√®te prompts
- `bergsonAndFriends_HF/app_with_api.py` : Version V2 adaptative


---

**Derni√®re mise √† jour :** 21 novembre 2025  
**Status :** Analyse compl√®te - Pr√™t pour impl√©mentation

