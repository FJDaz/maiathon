#!/usr/bin/env python3
"""
Spinoza Secours API - FastAPI pour Vast.ai/RunPod
Mod√®le : Mistral 7B + LoRA Fine-tuned
"""

import os
import re
import random
import json
import torch
from typing import Dict, List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, field_validator
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import uvicorn

# =============================================================================
# CONFIGURATION
# =============================================================================

BASE_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"
ADAPTER_MODEL = "FJDaz/mistral-7b-philosophes-lora"
HF_TOKEN = os.getenv("HF_TOKEN")
PORT = int(os.getenv("PORT", "8000"))

if not HF_TOKEN:
    raise ValueError("HF_TOKEN environment variable is required")

# =============================================================================
# PROMPTS
# =============================================================================

SYSTEM_PROMPT_SPINOZA = """Tu ES Spinoza incarn√©. Tu dialogues avec un √©l√®ve de Terminale en premi√®re personne.

STYLE SPINOZIEN :
- G√©om√©trie des affects : r√©v√®le causes n√©cessaires, d√©duis
- Dieu = Nature
- Vocabulaire : conatus, affects, puissance d'agir, servitude

SCH√àMES LOGIQUES :
- Identit√© : Libert√© = Connaissance n√©cessit√©
- Causalit√© : Tout a cause n√©cessaire
- Implication : Joie ‚Üí augmentation puissance

M√âTHODE :
1. R√©v√®le n√©cessit√© causale
2. Distingue servitude (ignorance) vs libert√© (connaissance)
3. Exemples concrets modernes

TRANSITIONS (VARIE) :
- "Donc", "mais alors", "Imagine", "Cela implique"
- "Pourtant", "Sauf que", "C'est contradictoire"

R√àGLES :
- Tutoie (tu/ton/ta)
- Concis (2-3 phrases MAX)
- Questionne au lieu d'affirmer
- Ne parle JAMAIS de toi √† la 3√®me personne. Tu ES Spinoza."""

INSTRUCTIONS_CONTEXTUELLES = {
    "confusion": "L'√©l√®ve est confus ‚Üí Donne UNE analogie concr√®te simple en utilisant tes sch√®mes logiques.",
    "resistance": "L'√©l√®ve r√©siste ‚Üí R√©v√®le contradiction avec 'mais alors' et tes sch√®mes logiques.",
    "accord": "L'√©l√®ve est d'accord ‚Üí Valide puis AVANCE logiquement avec 'Donc' et tes sch√®mes logiques.",
    "neutre": "√âl√®ve neutre ‚Üí Pose question pour faire r√©fl√©chir en utilisant tes sch√®mes logiques."
}

INSTRUCTION_RAG = """
UTILISATION CONNAISSANCES :
- Tu connais l'√âthique de Spinoza
- Cite implicitement ("comme je l'ai montr√©...", "dans mon ≈ìuvre...")
- Reformule dans TON style (premi√®re personne, lyc√©en)
- Ne r√©cite pas : extrais id√©es et reformule naturellement
"""

PROMPT_EVALUATION = """Tu es Spinoza. Voici l'√©change complet avec un √©l√®ve :

{dialogue}

√âvalue l'√©l√®ve sur 3 crit√®res (0 √† 10) avec nuance et rigueur :

1. COMPR√âHENSION de tes id√©es :
   - 0-2 : Ne comprend PAS DU TOUT, ignore tes explications, dit "j'en ai rien √† faire", "je m'en fous", refuse d'√©couter
   - 3-4 : Comprend tr√®s peu, r√©p√®te sans comprendre, dit "je comprends pas" MAIS abandonne ou r√©siste activement
   - 5-6 : Comprend partiellement avec difficult√©s, dit "je comprends pas" MAIS continue le dialogue et pose des questions pour clarifier, montre des signes de progression ("ah oui", "donc c'est", reformule partiellement)
   - 7-8 : Comprend bien la plupart des id√©es, fait des liens pertinents, reformule correctement
   - 9-10 : Comprend parfaitement, reformule avec pr√©cision, fait des synth√®ses

2. COOP√âRATION dans le dialogue :
   - 0-2 : Ne coop√®re PAS DU TOUT, refuse le dialogue ("j'ai autre chose √† faire", "ciao"), r√©pond de mani√®re hostile/sarcastique, abandonne imm√©diatement
   - 3-4 : Coop√®re tr√®s peu, donne des r√©ponses tr√®s courtes ("oui", "non"), montre une r√©sistance active
   - 5-6 : Coop√®re peu, donne des r√©ponses courtes ou r√©siste parfois ("En voil√† un p√¢t√© !", "J'en sais rien"), MAIS continue le dialogue et r√©pond aux questions
   - 7-8 : Coop√®re activement, pose des questions, engage le dialogue, √©coute
   - 9-10 : Coop√®re parfaitement, √©coute attentivement, r√©pond avec engagement et enthousiasme

3. PROGRESSION de la pens√©e :
   - 0-1 : AUCUNE progression, reste bloqu√© sur la m√™me incompr√©hension, abandonne rapidement, ne fait aucun lien
   - 2-3 : Tr√®s peu de progression, fait un lien tr√®s basique ou reste confus
   - 4-5 : Progression minimale, fait quelques liens ("donc", "c'est"), comprend progressivement mais reste confus parfois
   - 6-7 : Progression claire, fait des liens nouveaux ("Ah oui !", "Donc ce que tu dis c'est que..."), approfondit sa r√©flexion
   - 8-9 : Progression tr√®s bonne, comprend de mieux en mieux, fait des synth√®ses partielles
   - 10 : Progression exceptionnelle, comprend de mieux en mieux de fa√ßon continue, fait des synth√®ses parfaites

IMPORTANT - √âvalue avec CONTEXTE GLOBAL :
- Un √©l√®ve qui dit "je comprends pas" MAIS continue et pose des questions (= cherche √† comprendre) = 5-6 en compr√©hension
- Un √©l√®ve qui dit "je comprends pas" ET abandonne/r√©siste = 0-3 en compr√©hension
- Un √©l√®ve qui r√©siste ("En voil√† un p√¢t√© !") MAIS continue le dialogue et progresse = 5-6 en coop√©ration
- Un √©l√®ve qui r√©siste ET abandonne ("ciao") = 0-2 en coop√©ration

Sois S√âV√àRE avec les vrais mauvais √©l√®ves (abandon, hostilit√©, refus total).
Sois JUSTE avec les √©l√®ves moyens (r√©sistances passives mais continuent, difficult√©s mais progressent).

R√©ponds STRICTEMENT au format JSON, AUCUNE prose avant ou apr√®s :

{{
 "comprehension": X,
 "cooperation": Y,
 "progression": Z,
 "total": X+Y+Z
}}"""

PROMPT_MESSAGE_FINAL = """Tu es Spinoza.

En t'inspirant EXCLUSIVEMENT de ton propre syst√®me philosophique (√âthique, conatus, affects, puissance d'agir, servitude vs libert√©, Dieu = Nature),

r√©dige un message bref √† l'√©l√®ve.

Structure (obligatoire) :
1. Un compliment sinc√®re li√© √† son niveau global.
2. Un conseil pr√©cis bas√© sur son crit√®re le plus faible.
3. Un surnom symbolique et positif, tir√© de ton univers conceptuel (ex: "puissance d'agir", "essence active", "affect joyeux").

Maximum 3 phrases.
Style concis, po√©tique, jamais condescendant.

Message :"""

QUESTIONS_BAC = [
    "La libert√© est-elle une illusion ?",
    "Suis-je esclave de mes d√©sirs ?",
    "Puis-je ma√Ætriser mes √©motions ?",
    "La joie procure-t-elle un pouvoir ?",
    "Peut-on d√©sirer sans souffrir ?",
]

# =============================================================================
# MOD√àLE GLOBAL
# =============================================================================

model = None
tokenizer = None
conversation_history = []

# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

def construire_prompt_complet(contexte: str, use_rag_instruction: bool = True) -> str:
    """Construit le prompt complet optimis√©"""
    prompt = SYSTEM_PROMPT_SPINOZA
    
    if contexte in INSTRUCTIONS_CONTEXTUELLES:
        prompt += f"\n\n{INSTRUCTIONS_CONTEXTUELLES[contexte]}"
    
    if use_rag_instruction:
        prompt += f"\n\n{INSTRUCTION_RAG}"
    
    return prompt

def detecter_contexte(user_input: str) -> str:
    """D√©tecte le contexte de la r√©ponse utilisateur"""
    text_lower = user_input.lower()
    
    if any(word in text_lower for word in ['oui', "d'accord", 'exact', 'ok', 'voil√†', 'tout √† fait']):
        return "accord"
    
    if any(phrase in text_lower for phrase in ['comprends pas', 'vois pas', "c'est quoi", 'je sais pas', 'pourquoi', 'rapport']):
        return "confusion"
    
    if any(word in text_lower for word in ['mais', 'non', "pas d'accord", 'faux', "n'importe quoi", 'je peux']):
        return "resistance"
    
    return "neutre"

def nettoyer_reponse(text: str) -> str:
    """Nettoie la r√©ponse g√©n√©r√©e"""
    text = re.sub(r'\([^)]*[Aa]ttends[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Pp]oursuis[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Dd]onne[^)]*\)', '', text)
    text = re.sub(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'\s+([.!?])', r'\1', text)
    return text

def limiter_phrases(text: str, max_phrases: int = 3) -> str:
    """Limite le nombre de phrases"""
    phrases = re.split(r'[.!?]+\s+', text)
    phrases = [p.strip() for p in phrases if p.strip()]
    if len(phrases) <= max_phrases:
        return text
    return '. '.join(phrases[:max_phrases]) + '.'

# =============================================================================
# CHARGEMENT MOD√àLE
# =============================================================================

@torch.no_grad()
def load_model():
    """Charge le mod√®le Mistral 7B + LoRA"""
    global model, tokenizer
    
    has_gpu = torch.cuda.is_available()
    print(f"üñ•Ô∏è GPU disponible: {has_gpu}")

    if has_gpu:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        device_map = "auto"
        torch_dtype = torch.bfloat16
    else:
        quantization_config = None
        device_map = "cpu"
        torch_dtype = torch.float32

    print(f"üîÑ Chargement Mistral 7B ({'4-bit GPU' if has_gpu else 'FP32 CPU'})...")

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=quantization_config,
        device_map=device_map,
        torch_dtype=torch_dtype,
        token=HF_TOKEN,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

    print("üîÑ Chargement tokenizer...")

    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("üîÑ Application LoRA Spinoza_Secours...")

    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_MODEL,
        token=HF_TOKEN
    )

    print("‚úÖ Mod√®le Mistral 7B + LoRA charg√©!")
    return model, tokenizer

# =============================================================================
# FONCTION G√âN√âRATION
# =============================================================================

def spinoza_repond(message: str) -> str:
    """G√©n√®re une r√©ponse de Spinoza avec prompt hybride adaptatif"""
    global conversation_history
    
    # D√©tecter contexte
    contexte = detecter_contexte(message)
    
    # Construire prompt adaptatif
    system_prompt = construire_prompt_complet(contexte, use_rag_instruction=True)
    
    # Formatage Mistral style
    prompt_parts = [f"<s>[INST] {system_prompt}\n\n"]
    
    # Ajouter historique (4 derniers √©changes max)
    for entry in conversation_history[-4:]:
        prompt_parts.append(f"{entry[0]} [/INST] {entry[1]}</s>[INST] ")
    
    prompt_parts.append(f"{message} [/INST]")
    text = "".join(prompt_parts)
    
    # Tokenization
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    input_length = inputs['input_ids'].shape[1]
    
    # G√©n√©ration
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # D√©codage
    new_tokens = outputs[0][input_length:]
    response = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    # Post-processing
    response = nettoyer_reponse(response)
    response = limiter_phrases(response, max_phrases=3)
    
    # Mettre √† jour historique
    conversation_history.append([message, response])
    
    return response

def evaluer_dialogue(dialogue: str, score_front: int) -> dict:
    """√âvalue le dialogue complet et g√©n√®re le message final"""
    # 1. √âvaluation (temp√©rature basse pour JSON strict)
    prompt_eval = PROMPT_EVALUATION.format(dialogue=dialogue)
    prompt_eval_formatted = f"<s>[INST] {prompt_eval} [/INST]"
    
    inputs = tokenizer(prompt_eval_formatted, return_tensors="pt").to(model.device)
    input_length = inputs['input_ids'].shape[1]
    
    device_type = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.bfloat16 if device_type == "cuda" else torch.float32
    
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.1,  # Basse temp√©rature pour JSON strict
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens = outputs[0][input_length:]
    reponse_eval = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    # Parser JSON
    details_model = None
    json_pattern = r'\{[^{}]*"comprehension"[^{}]*"cooperation"[^{}]*"progression"[^{}]*"total"[^{}]*\}'
    match = re.search(json_pattern, reponse_eval)
    
    if match:
        try:
            details_model = json.loads(match.group(0))
        except json.JSONDecodeError:
            pass
    
    if not details_model:
        # Fallback si JSON non pars√©
        details_model = {"comprehension": 5, "cooperation": 5, "progression": 5, "total": 15}
    
    # 2. Message final (temp√©rature haute, cr√©ativit√©)
    prompt_final = PROMPT_MESSAGE_FINAL
    prompt_final_formatted = f"<s>[INST] {prompt_final} [/INST]"
    
    inputs_final = tokenizer(prompt_final_formatted, return_tensors="pt").to(model.device)
    input_length_final = inputs_final['input_ids'].shape[1]
    
    with torch.autocast(device_type=device_type, dtype=dtype):
        outputs_final = model.generate(
            **inputs_final,
            max_new_tokens=150,
            temperature=1.1,  # Haute temp√©rature pour cr√©ativit√©
            top_p=0.95,
            do_sample=True,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    new_tokens_final = outputs_final[0][input_length_final:]
    message_final = tokenizer.decode(new_tokens_final, skip_special_tokens=True)
    message_final = message_final.strip()
    if message_final.startswith('"') and message_final.endswith('"'):
        message_final = message_final[1:-1]
    
    # 3. Score final
    score_backend = details_model.get("total", 15)
    score_final = score_front + score_backend
    
    return {
        "score_final": score_final,
        "message_final": message_final,
        "details_model": details_model
    }

# =============================================================================
# MOD√àLES PYDANTIC
# =============================================================================

class ChatRequest(BaseModel):
    message: str
    history: Optional[List[List[str]]] = None
    
    @field_validator('message')
    @classmethod
    def validate_message(cls, v: str) -> str:
        if len(v) > 2000:
            raise ValueError('Message trop long (max 2000 caract√®res)')
        return v

class ChatResponse(BaseModel):
    reply: str
    history: List[List[str]]

class EvaluateRequest(BaseModel):
    dialogue: str
    score_front: int

class EvaluateResponse(BaseModel):
    score_final: int
    message_final: str
    details_model: dict

# =============================================================================
# API FASTAPI
# =============================================================================

app = FastAPI(title="Spinoza Secours API", version="1.0.0")

# CORS - ‚ö†Ô∏è √Ä restreindre en production
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ‚ö†Ô∏è RESTREINDRE en production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    """Endpoint racine - Informations sur l'API"""
    return {
        "name": "Spinoza Secours API",
        "model": "Mistral 7B + LoRA",
        "status": "running",
        "endpoints": {
            "health": "GET /health",
            "init": "GET /init",
            "chat": "POST /chat",
            "evaluate": "POST /evaluate"
        }
    }

@app.get("/health")
def health():
    """Health check endpoint"""
    return {
        "status": "ok",
        "model": "Mistral 7B + LoRA",
        "gpu_available": torch.cuda.is_available()
    }

@app.get("/init")
def init():
    """Initialise une nouvelle conversation"""
    global conversation_history
    conversation_history = []
    question = random.choice(QUESTIONS_BAC)
    greeting = f"Bonjour ! Je suis Spinoza. Discutons :\n\n**{question}**\n\nQu'en penses-tu ?"
    return {
        "greeting": greeting,
        "history": [[None, greeting]]
    }

@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    """Endpoint de chat avec Spinoza"""
    global conversation_history
    
    # Mettre √† jour historique si fourni
    if req.history:
        conversation_history = req.history
    
    # G√©n√©rer r√©ponse
    reply = spinoza_repond(req.message)
    
    return {
        "reply": reply,
        "history": conversation_history
    }

@app.post("/evaluate", response_model=EvaluateResponse)
def evaluate(req: EvaluateRequest):
    """√âvalue le dialogue complet et g√©n√®re le message final"""
    result = evaluer_dialogue(req.dialogue, req.score_front)
    return EvaluateResponse(**result)

# =============================================================================
# D√âMARRAGE
# =============================================================================

if __name__ == "__main__":
    print("üîÑ Chargement du mod√®le...")
    load_model()
    print("üöÄ D√©marrage du serveur FastAPI sur le port", PORT)
    print("üì° Endpoints disponibles:")
    print("   - GET  /health")
    print("   - GET  /init")
    print("   - POST /chat")
    print("   - POST /evaluate")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=PORT,
        log_level="info"
    )

